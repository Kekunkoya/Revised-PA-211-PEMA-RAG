{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems\n",
    "\n",
    "This notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information.\n",
    "\n",
    "## Key Transformation Techniques\n",
    "\n",
    "1. **Query Rewriting**: Makes queries more specific and detailed for better search precision.\n",
    "2. **Step-back Prompting**: Generates broader queries to retrieve useful contextual information.\n",
    "3. **Sub-query Decomposition**: Breaks complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Query Transformation Techniques\n",
    "### 1. Query Rewriting\n",
    "This technique makes queries more specific and detailed to improve precision in retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for query rewriting\n",
    "        \n",
    "    Returns:\n",
    "        str: The rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be rewritten\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the rewritten query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting\n",
    "This technique generates broader queries to retrieve contextual background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates a more general 'step-back' query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for step-back query generation\n",
    "        \n",
    "    Returns:\n",
    "        str: The step-back query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be generalized\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the step-back query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the step-back query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition\n",
    "This technique breaks down complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original complex query\n",
    "        num_subqueries (int): Number of sub-queries to generate\n",
    "        model (str): The model to use for query decomposition\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be decomposed\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the sub-queries using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.4,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Process the response to extract sub-queries\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numbered queries using simple parsing\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # Remove the number and leading space\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Transformation Techniques\n",
    "Let's apply these techniques to an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What instructions does the PEMA Home Damage Assessment Guide give for reporting home damage after a storm?\n",
      "\n",
      "1. Rewritten Query:\n",
      "What specific guidelines and procedures does the Pennsylvania Emergency Management Agency (PEMA) Home Damage Assessment Guide provide for homeowners to accurately report property damage following a severe storm event? Please include details on required documentation, assessment criteria, and any deadlines for reporting.\n",
      "\n",
      "2. Step-back Query:\n",
      "What are the general guidelines and procedures for assessing and reporting home damage after a storm?\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. What specific steps are outlined in the PEMA Home Damage Assessment Guide for assessing home damage after a storm?\n",
      "   2. What information is required to be collected and reported according to the PEMA Home Damage Assessment Guide?\n",
      "   3. Are there any deadlines or timeframes mentioned in the PEMA Home Damage Assessment Guide for reporting home damage after a storm?\n",
      "   4. What resources or contacts does the PEMA Home Damage Assessment Guide recommend for assistance in reporting home damage?\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "original_query = \"What instructions does the PEMA Home Damage Assessment Guide give for reporting home damage after a storm?\"\n",
    "\n",
    "# Apply query transformations\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "To demonstrate how query transformations integrate with retrieval, let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings as a list of vectors\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Includes a SimpleVectorStore class that uses NumPy for in-memory storage of text, embeddings, and metadata, enabling basic cosine similarity searches.\n",
    "It calculates cosine similarity between the query_vector and all stored vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PA 211 Disaster Community Resources.pdf ---\n",
      "PA 211 Community Disaster and Human \n",
      "Services Resources in Pennsylvania \n",
      "Introduction \n",
      " \n",
      "Community Disaster and Human Services Resources in Pennsylvania \n",
      " \n",
      "Disasters, whether natural or man-made, have significant and far-reaching impacts on \n",
      "individuals, families, and communities. Pennsylvania, with its mix of urban, suburban, and \n",
      "rural regions, faces a diverse array of emergencies ranging from floods and severe storms to \n",
      "public health crises and housing instability. To ensure an effective res\n",
      "\n",
      "--- 211 RESPONDS TO URGENT NEEDS.pdf ---\n",
      "211 RESPONDS TO URGENT NEEDS \n",
      "FACT\n",
      "211 stood up a statewide text\n",
      "response to support employees\n",
      "impacted by the partial federal\n",
      "government shutdown who did\n",
      "not know when they would\n",
      "receive their next paycheck.\n",
      "211 assists in times of\n",
      "disaster and widespread\n",
      "need\n",
      "FACT\n",
      "FACT\n",
      "1\n",
      "PLEASE VOTE TO INCLUDE FUNDING FOR PENNSYLVANIA'S 211 SYSTEM IN THE STATE BUDGET TO\n",
      "SUPPORT 211'S CAPACITY TO HELP OUR COMMUNITIES IN TIMES OF DISASTER OR GREAT NEED.\n",
      "1 2-1-1 Data\n",
      "In January 2019, 187 individuals\n",
      "subscribed to\n",
      "\n",
      "--- PEMA.pdf ---\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESS\n",
      "GUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready.PA.gov \n",
      "readypa@pa.gov\n",
      "Emergency Preparedness Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page \n",
      "\n",
      "--- ready-gov_disaster-preparedness-guide-for-older-adults.pdf ---\n",
      "1\n",
      "TAKE  \n",
      "CONTROL IN\n",
      "1\n",
      "2\n",
      "3\n",
      "Disaster Preparedness Guide for Older Adults\n",
      " \n",
      " \n",
      "STEP 1 | ASSESS YOUR NEEDS\n",
      "First, know your risk. Then, understand your needs during emergencies. \n",
      "This section guides you through a self-assessment process to identify your \n",
      "specific needs so that you can create a personalized emergency plan.\n",
      "STEP 2 | MAKE A PLAN\n",
      "Develop a comprehensive emergency plan and emergency \n",
      "preparedness kit tailored to your unique needs. This section ensures \n",
      "you are well prepared to respond to \n",
      "\n",
      "--- Substantial Damages Toolkit.pdf ---\n",
      " \n",
      " \n",
      " \n",
      "Prepared for: \n",
      "Pennsylvania Emergency Management \n",
      "Agency \n",
      "Emergency Management, Mitigation, \n",
      "Insurance, and Resilient Communities \n",
      "(MIRC) Office \n",
      "1310 Elmerton Avenue  \n",
      "Harrisburg, Pennsylvania 17110 \n",
      "Prepared by: \n",
      "PG Environmental and ERG \n",
      "14555 Avion Parkway, Suite 125 \n",
      "Chantilly, VA 20151 \n",
      " \n",
      "Substantial Improvements /  \n",
      "Substantial Damages Toolkit \n",
      "APRIL 2023 \n",
      " \n",
      "ii \n",
      " \n",
      "Contents \n",
      "Introduction .................................................................................................\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_texts_from_folder(pdf_folder):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in a folder.\n",
    "\n",
    "    Args:\n",
    "        pdf_folder (str): Path to the folder containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with filenames as keys and extracted text as values.\n",
    "    \"\"\"\n",
    "    pdf_texts = {}\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            mypdf = fitz.open(pdf_path)\n",
    "            all_text = \"\"\n",
    "            for page_num in range(mypdf.page_count):\n",
    "                page = mypdf[page_num]\n",
    "                text = page.get_text(\"text\")\n",
    "                all_text += text\n",
    "            mypdf.close()\n",
    "            pdf_texts[filename] = all_text\n",
    "    return pdf_texts\n",
    "\n",
    "# Example usage:\n",
    "pdf_folder = '/Users/kekunkoya/Desktop/Revised RAG Project/PDFs'  \n",
    "all_pdf_texts = extract_texts_from_folder(pdf_folder)\n",
    "\n",
    "# To print a preview of each:\n",
    "for fname, text in all_pdf_texts.items():\n",
    "    print(f\"\\n--- {fname} ---\")\n",
    "    print(text[:500])  # Print first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PA 211 Disaster Community Resources.pdf ---\n",
      "PA 211 Community Disaster and Human \n",
      "Services Resources in Pennsylvania \n",
      "Introduction \n",
      " \n",
      "Community Disaster and Human Services Resources in Pennsylvania \n",
      " \n",
      "Disasters, whether natural or man-made, have significant and far-reaching impacts on \n",
      "individuals, families, and communities. Pennsylvania, with its mix of urban, suburban, and \n",
      "rural regions, faces a diverse array of emergencies ranging from floods and severe storms to \n",
      "public health crises and housing instability. To ensure an effective res\n",
      "----------------------------------------\n",
      "\n",
      "--- 211 RESPONDS TO URGENT NEEDS.pdf ---\n",
      "211 RESPONDS TO URGENT NEEDS \n",
      "FACT\n",
      "211 stood up a statewide text\n",
      "response to support employees\n",
      "impacted by the partial federal\n",
      "government shutdown who did\n",
      "not know when they would\n",
      "receive their next paycheck.\n",
      "211 assists in times of\n",
      "disaster and widespread\n",
      "need\n",
      "FACT\n",
      "FACT\n",
      "1\n",
      "PLEASE VOTE TO INCLUDE FUNDING FOR PENNSYLVANIA'S 211 SYSTEM IN THE STATE BUDGET TO\n",
      "SUPPORT 211'S CAPACITY TO HELP OUR COMMUNITIES IN TIMES OF DISASTER OR GREAT NEED.\n",
      "1 2-1-1 Data\n",
      "In January 2019, 187 individuals\n",
      "subscribed to\n",
      "----------------------------------------\n",
      "\n",
      "--- PEMA.pdf ---\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESS\n",
      "GUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready.PA.gov \n",
      "readypa@pa.gov\n",
      "\n",
      "Emergency Preparedness Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page\n",
      "----------------------------------------\n",
      "\n",
      "--- ready-gov_disaster-preparedness-guide-for-older-adults.pdf ---\n",
      "1\n",
      "TAKE  \n",
      "CONTROL IN\n",
      "1\n",
      "2\n",
      "3\n",
      "Disaster Preparedness Guide for Older Adults\n",
      "\n",
      " \n",
      " \n",
      "STEP 1 | ASSESS YOUR NEEDS\n",
      "First, know your risk. Then, understand your needs during emergencies. \n",
      "This section guides you through a self-assessment process to identify your \n",
      "specific needs so that you can create a personalized emergency plan.\n",
      "STEP 2 | MAKE A PLAN\n",
      "Develop a comprehensive emergency plan and emergency \n",
      "preparedness kit tailored to your unique needs. This section ensures \n",
      "you are well prepared to respond to\n",
      "----------------------------------------\n",
      "\n",
      "--- Substantial Damages Toolkit.pdf ---\n",
      " \n",
      " \n",
      " \n",
      "Prepared for: \n",
      "Pennsylvania Emergency Management \n",
      "Agency \n",
      "Emergency Management, Mitigation, \n",
      "Insurance, and Resilient Communities \n",
      "(MIRC) Office \n",
      "1310 Elmerton Avenue  \n",
      "Harrisburg, Pennsylvania 17110 \n",
      "Prepared by: \n",
      "PG Environmental and ERG \n",
      "14555 Avion Parkway, Suite 125 \n",
      "Chantilly, VA 20151 \n",
      " \n",
      "Substantial Improvements /  \n",
      "Substantial Damages Toolkit \n",
      "APRIL 2023 \n",
      "\n",
      " \n",
      "ii \n",
      " \n",
      "Contents \n",
      "Introduction ................................................................................................\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_texts_from_folder(pdf_folder: str):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDFs in a folder and returns a dictionary.\n",
    "    Args:\n",
    "        pdf_folder (str): Path to the folder containing PDF files.\n",
    "    Returns:\n",
    "        dict: {filename: extracted_text}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            results[filename] = text\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "pdf_folder = \"/Users/kekunkoya/Desktop/Revised RAG Project/PDFs\"  \n",
    "all_texts = extract_texts_from_folder(pdf_folder)\n",
    "\n",
    "# Print the first 500 characters of each PDF's extracted text\n",
    "for fname, text in all_texts.items():\n",
    "    print(f\"\\n--- {fname} ---\")\n",
    "    print(text[:500])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    # Create embeddings for all chunks at once for efficiency\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add chunks to vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    Search using a transformed query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with rewritten query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # Step-back prompting\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with step-back query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # Sub-query decomposition\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # Create embeddings for all sub-queries\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # Search with each sub-query and combine results\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # Remove duplicates (keep highest similarity score)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # Sort by similarity and take top_k\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # Regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response with Transformed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): The model to use for response generation\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
    "    \n",
    "    # Define the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the generated response, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, transformed query, context, and response\n",
    "    \"\"\"\n",
    "    # Process the document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Apply query transformation and search\n",
    "    if transformation_type:\n",
    "        # Perform search with transformed query\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # Perform regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # Combine context from search results\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # Generate response based on the query and combined context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the results including original query, transformation type, context, and response\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_responses(results, reference_answer, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Compare responses from different query transformation techniques.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from different transformation techniques\n",
    "        reference_answer (str): Reference answer for comparison\n",
    "        model (str): Model for evaluation\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "    \n",
    "    # Prepare the comparison text with the reference answer and responses from each technique\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "    # Define the user prompt with the comparison text\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the evaluation response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate different transformation techniques for the same query.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): Query to evaluate\n",
    "        reference_answer (str): Optional reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Define the transformation techniques to evaluate\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # Run RAG with each transformation technique\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "        # Get the result for the current transformation type\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # Print the response for the current transformation type\n",
    "        print(f\"Response with {type_name} query:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Compare results if a reference answer is provided\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 12 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 12 chunks to the vector store\n",
      "Response with original query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 12 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 12 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Rewritten query: What specific steps and resources does the Pennsylvania Emergency Management Agency (PEMA) Disaster Resource Guide recommend for locating emergency food assistance in Harrisburg, Pennsylvania (ZIP code 17104) following a natural disaster? Please include information on local food banks, government assistance programs, and any relevant contact details or websites.\n",
      "Response with rewrite query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 12 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 12 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Step-back query: What are the general guidelines and resources for accessing emergency food assistance after a natural disaster in urban areas?\n",
      "Response with step_back query:\n",
      "The provided context does not include specific steps recommended by the PEMA Disaster Resource Guide for finding emergency food after a natural disaster in Harrisburg (ZIP 17104). Therefore, I don't have enough information to answer your question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 12 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 12 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Decomposed into sub-queries:\n",
      "1. What are the key resources listed in the PEMA Disaster Resource Guide for locating emergency food in Harrisburg (ZIP 17104) after a natural disaster?\n",
      "2. What specific steps does the PEMA Disaster Resource Guide suggest for assessing immediate food needs following a natural disaster?\n",
      "3. Are there any local organizations or agencies mentioned in the PEMA Disaster Resource Guide that provide emergency food assistance in Harrisburg (ZIP 17104)?\n",
      "4. What information does the PEMA Disaster Resource Guide provide about the hours of operation and locations of food distribution sites in Harrisburg (ZIP 17104) after a disaster?\n",
      "Response with decompose query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Original Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** The response does not provide any information relevant to the query. It fails to address the question about emergency food distribution in Harrisburg, showing a complete lack of engagement with the content of the reference answer.\n",
      "\n",
      "#### 2. Rewrite Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response is identical to the original query response and suffers from the same issues. It does not provide any relevant information or attempt to answer the question.\n",
      "\n",
      "#### 3. Step_back Query Response\n",
      "- **Score:** 3/10\n",
      "- **Strengths:** Acknowledges the lack of specific information in the provided context.\n",
      "- **Weaknesses:** While it attempts to explain the absence of information, it still does not provide any actionable guidance or relevant details from the reference answer. It fails to engage with the query meaningfully.\n",
      "\n",
      "#### 4. Decompose Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response is identical to the original and rewrite responses, providing no relevant information or engagement with the query.\n",
      "\n",
      "### Ranking of Techniques\n",
      "1. **Step_back Query Response** (Score: 3/10)\n",
      "2. **Original Query Response** (Score: 1/10)\n",
      "3. **Rewrite Query Response** (Score: 1/10)\n",
      "4. **Decompose Query Response** (Score: 1/10)\n",
      "\n",
      "### Overall Performance\n",
      "The **Step_back Query Response** performed the best overall, despite its low score, because it at least acknowledged the lack of specific information in the context provided. The other responses (original, rewrite, and decompose) were all identical and failed to provide any relevant information or engagement with the query, resulting in the lowest possible scores. \n",
      "\n",
      "In summary, while none of the responses effectively addressed the query, the step-back technique at least attempted to contextualize the lack of information, making it slightly more relevant than the others.\n",
      "=============================\n",
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 7 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 7 chunks to the vector store\n",
      "Response with original query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 7 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 7 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Rewritten query: What specific steps and resources does the Pennsylvania Emergency Management Agency (PEMA) Disaster Resource Guide recommend for locating emergency food assistance in Harrisburg, Pennsylvania (ZIP code 17104) following a natural disaster? Please include information on local food banks, government assistance programs, and any relevant contact details or websites.\n",
      "Response with rewrite query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 7 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 7 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Step-back query: What are the general guidelines and resources for accessing emergency food assistance after a natural disaster in urban areas?\n",
      "Response with step_back query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 7 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 7 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Decomposed into sub-queries:\n",
      "1. What specific resources or organizations does the PEMA Disaster Resource Guide list for obtaining emergency food in Harrisburg (ZIP 17104)?\n",
      "2. What are the recommended steps for assessing immediate food needs after a natural disaster according to the PEMA Disaster Resource Guide?\n",
      "3. How can individuals in Harrisburg (ZIP 17104) locate food distribution sites or shelters that provide emergency food assistance post-disaster?\n",
      "4. What additional support services related to food access does the PEMA Disaster Resource Guide suggest for residents affected by a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Response with decompose query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Original Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** The response does not provide any information relevant to the query. It fails to address the question about emergency food distribution sites in Harrisburg, which is critical information for residents in need.\n",
      "\n",
      "#### 2. Rewrite Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response is identical to the original query response and does not provide any relevant information. It fails to address the query effectively.\n",
      "\n",
      "#### 3. Step_back Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Similar to the previous responses, this one does not provide any relevant information. It simply repeats the lack of information without attempting to address the query.\n",
      "\n",
      "#### 4. Decompose Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response also fails to provide any relevant information. It does not attempt to break down the query or provide any insights into the emergency food distribution sites.\n",
      "\n",
      "### Ranking of Techniques\n",
      "1. **Original Query Response** - Score: 1/10\n",
      "2. **Rewrite Query Response** - Score: 1/10\n",
      "3. **Step_back Query Response** - Score: 1/10\n",
      "4. **Decompose Query Response** - Score: 1/10\n",
      "\n",
      "### Overall Performance\n",
      "All techniques performed equally poorly, scoring a 1 out of 10. None of the responses provided any relevant information or addressed the query about emergency food distribution sites in Harrisburg. \n",
      "\n",
      "### Conclusion\n",
      "Since all responses are identical and lack any useful content, there is no clear winner among the techniques. They all failed to meet the criteria of accuracy, completeness, and relevance. In this case, none of the query transformation techniques were effective in generating a useful response.\n",
      "=============================\n",
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 68 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 68 chunks to the vector store\n",
      "Response with original query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 68 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 68 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Rewritten query: What specific steps and resources does the Pennsylvania Emergency Management Agency (PEMA) Disaster Resource Guide recommend for locating emergency food assistance in Harrisburg, Pennsylvania (ZIP code 17104) following a natural disaster? Please include information on local food banks, government assistance programs, and community organizations involved in disaster relief efforts.\n",
      "Response with rewrite query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 68 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 68 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Step-back query: What are the general recommendations for accessing emergency food resources after a natural disaster in urban areas?\n",
      "Response with step_back query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 68 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 68 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Decomposed into sub-queries:\n",
      "1. What specific resources does the PEMA Disaster Resource Guide provide for locating emergency food services in Harrisburg (ZIP 17104) after a natural disaster?\n",
      "2. What types of emergency food assistance programs are available in Harrisburg (ZIP 17104) according to the PEMA Disaster Resource Guide?\n",
      "3. What steps does the PEMA Disaster Resource Guide suggest for assessing individual or family food needs after a natural disaster?\n",
      "4. How can residents of Harrisburg (ZIP 17104) access information about local food distribution sites following a natural disaster as recommended by the PEMA Disaster Resource Guide?\n",
      "Response with decompose query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Original Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** The response does not provide any information relevant to the query. It fails to address the question about emergency food distribution sites in Harrisburg, making it completely unhelpful.\n",
      "\n",
      "#### 2. Rewrite Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Similar to the original response, it does not provide any information or attempt to answer the query. The response is identical to the original, showing no improvement or additional context.\n",
      "\n",
      "#### 3. Step_back Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response is also identical to the original and rewrite, offering no information or insight into the query. It fails to engage with the question at all.\n",
      "\n",
      "#### 4. Decompose Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Like the previous responses, this one does not provide any relevant information. It merely states a lack of information without attempting to clarify or break down the query.\n",
      "\n",
      "### Ranking of Techniques\n",
      "1. **Original Query Response:** 1/10\n",
      "2. **Rewrite Query Response:** 1/10\n",
      "3. **Step_back Query Response:** 1/10\n",
      "4. **Decompose Query Response:** 1/10\n",
      "\n",
      "### Overall Performance\n",
      "All techniques performed equally poorly, scoring 1 out of 10. None of the responses provided any relevant information or attempted to answer the query regarding emergency food distribution sites in Harrisburg. They all expressed a lack of information without any effort to guide the user toward finding the needed resources.\n",
      "\n",
      "### Conclusion\n",
      "Since all responses were identical and unhelpful, there is no clear winner among the techniques. They all failed to meet the criteria of accuracy, completeness, and relevance. In a practical scenario, none of these techniques would be suitable for providing a useful response to the query. A better approach would involve generating a response that includes specific information about contacting PA 211 and checking local resources, as outlined in the reference answer.\n",
      "=============================\n",
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 23 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 23 chunks to the vector store\n",
      "Response with original query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 23 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 23 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Rewritten query: What specific steps and resources does the Pennsylvania Emergency Management Agency (PEMA) Disaster Resource Guide recommend for locating emergency food assistance in Harrisburg, Pennsylvania (ZIP code 17104) following a natural disaster? Please include information on local food banks, shelters, and government assistance programs available in this area.\n",
      "Response with rewrite query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 23 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 23 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Step-back query: What are the general guidelines and resources for accessing emergency food assistance after a natural disaster in urban areas?\n",
      "Response with step_back query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 23 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 23 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Decomposed into sub-queries:\n",
      "1. What specific types of emergency food resources are listed in the PEMA Disaster Resource Guide for Harrisburg (ZIP 17104)?\n",
      "2. What are the recommended steps for locating these emergency food resources after a natural disaster according to the PEMA Disaster Resource Guide?\n",
      "3. Are there any local organizations or agencies mentioned in the PEMA Disaster Resource Guide that assist with emergency food distribution in Harrisburg (ZIP 17104)?\n",
      "4. What additional resources or contact information does the PEMA Disaster Resource Guide provide for individuals seeking emergency food assistance after a natural disaster?\n",
      "Response with decompose query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Original Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** The response does not provide any information relevant to the query. It fails to address the question about emergency food distribution sites in Harrisburg, making it completely unhelpful.\n",
      "\n",
      "#### 2. Rewrite Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Similar to the original response, it does not provide any relevant information. The addition of \"to answer that question\" does not contribute to the content or utility of the response.\n",
      "\n",
      "#### 3. Step_back Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response is identical to the original response and does not provide any information. It fails to address the query and is therefore unhelpful.\n",
      "\n",
      "#### 4. Decompose Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Like the previous responses, it does not provide any relevant information. The response is uninformative and does not address the query.\n",
      "\n",
      "### Ranking of Techniques\n",
      "1. **Original Query Response** - Score: 1/10\n",
      "2. **Rewrite Query Response** - Score: 1/10\n",
      "3. **Step_back Query Response** - Score: 1/10\n",
      "4. **Decompose Query Response** - Score: 1/10\n",
      "\n",
      "### Overall Performance\n",
      "All four techniques produced identical responses that were completely unhelpful and irrelevant to the query. They all received the same low score of 1/10 due to their lack of accuracy, completeness, and relevance. \n",
      "\n",
      "### Conclusion\n",
      "None of the techniques performed well, as they all failed to provide any useful information regarding the query about emergency food distribution sites in Harrisburg. Since all responses were equally ineffective, there is no clear winner among the techniques. The overall performance indicates a need for a more effective query transformation approach that can generate informative and relevant responses.\n",
      "=============================\n",
      "\n",
      "===== Running RAG with original query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 299 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 299 chunks to the vector store\n",
      "Response with original query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with rewrite query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 299 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 299 chunks to the vector store\n",
      "Transformation type: rewrite\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Rewritten query: What specific steps and resources does the Pennsylvania Emergency Management Agency (PEMA) Disaster Resource Guide recommend for locating emergency food assistance in Harrisburg, Pennsylvania (ZIP code 17104) following a natural disaster? Please include information on local food banks, government assistance programs, and community organizations involved in disaster relief efforts.\n",
      "Response with rewrite query:\n",
      "I don't have enough information to answer that question.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with step_back query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 299 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 299 chunks to the vector store\n",
      "Transformation type: step_back\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Step-back query: What are the general guidelines and resources for accessing emergency food assistance after a natural disaster in urban areas?\n",
      "Response with step_back query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== Running RAG with decompose query =====\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 299 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 299 chunks to the vector store\n",
      "Transformation type: decompose\n",
      "Original query: What steps does the PEMA Disaster Resource Guide recommend for finding emergency food after a natural disaster in Harrisburg (ZIP 17104)?\n",
      "Decomposed into sub-queries:\n",
      "1. What specific resources does the PEMA Disaster Resource Guide list for locating emergency food in Harrisburg (ZIP 17104) after a natural disaster?\n",
      "2. What are the recommended steps for assessing immediate food needs following a natural disaster according to the PEMA Disaster Resource Guide?\n",
      "3. Are there local organizations or agencies mentioned in the PEMA Disaster Resource Guide that provide emergency food assistance in Harrisburg (ZIP 17104)?\n",
      "4. What additional tips or advice does the PEMA Disaster Resource Guide offer for individuals seeking emergency food after a natural disaster?\n",
      "Response with decompose query:\n",
      "I don't have enough information.\n",
      "==================================================\n",
      "\n",
      "===== EVALUATION RESULTS =====\n",
      "### Evaluation of Responses\n",
      "\n",
      "#### 1. Original Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** The response does not provide any relevant information or attempt to address the query. It fails to acknowledge the specific needs of the residents of Harrisburg regarding emergency food distribution.\n",
      "\n",
      "#### 2. Rewrite Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Similar to the original response, it does not provide any relevant information. The response is identical to the original, indicating no improvement or attempt to engage with the query.\n",
      "\n",
      "#### 3. Step_back Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** This response is vague and does not provide any useful information. It fails to address the query about emergency food distribution in Harrisburg.\n",
      "\n",
      "#### 4. Decompose Query Response\n",
      "- **Score:** 1/10\n",
      "- **Strengths:** None.\n",
      "- **Weaknesses:** Like the previous responses, it does not provide any relevant information or context. It is also vague and unhelpful.\n",
      "\n",
      "### Ranking of Techniques\n",
      "1. **Original Query Response** - Score: 1/10\n",
      "2. **Rewrite Query Response** - Score: 1/10\n",
      "3. **Step_back Query Response** - Score: 1/10\n",
      "4. **Decompose Query Response** - Score: 1/10\n",
      "\n",
      "### Overall Performance\n",
      "All techniques performed equally poorly, scoring 1 out of 10. None of the responses provided any relevant information or addressed the query regarding emergency food distribution for residents of Harrisburg. \n",
      "\n",
      "### Conclusion\n",
      "Since all responses are identical in their lack of content and relevance, there is no clear winner among the techniques. They all failed to provide any useful information, demonstrating that the query transformation techniques used did not enhance the response quality. In this case, none of the techniques were effective, and a different approach is needed to generate a meaningful response.\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load the validation data from a JSON file\n",
    "with open('/Users/kekunkoya/Desktop/Revised RAG Project/PA211_expanded_dataset.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query and reference answer from the validation data\n",
    "query = data[0]['question']\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# PDF folder path\n",
    "pdf_folder = \"/Users/kekunkoya/Desktop/Revised RAG Project/PDFs\"\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "# Loop over every PDF in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        # Run evaluation on each PDF\n",
    "        result = evaluate_transformations(pdf_path, query, reference_answer)\n",
    "        evaluation_results.append({\n",
    "            \"pdf_file\": filename,\n",
    "            \"result\": result\n",
    "        })\n",
    "\n",
    "# Now evaluation_results will have results for every PDF in the folder!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
