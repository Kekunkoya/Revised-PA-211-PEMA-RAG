{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Adaptive Retrieval for Enhanced RAG Systems\n",
    "\n",
    "In this notebook, I implement an Adaptive Retrieval system that dynamically selects the most appropriate retrieval strategy based on the type of query. This approach significantly enhances our RAG system's ability to provide accurate and relevant responses across a diverse range of questions.\n",
    "\n",
    "Different questions demand different retrieval strategies. Our system:\n",
    "\n",
    "1. Classifies the query type (Factual, Analytical, Opinion, or Contextual)\n",
    "2. Selects the appropriate retrieval strategy\n",
    "3. Executes specialized retrieval techniques\n",
    "4. Generates a tailored response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PA 211 Disaster Community Resources.pdf ---\n",
      "PA 211 Community Disaster and Human \n",
      "Services Resources in Pennsylvania \n",
      "Introduction \n",
      " \n",
      "Community Disaster and Human Services Resources in Pennsylvania \n",
      " \n",
      "Disasters, whether natural or man-made, have significant and far-reaching impacts on \n",
      "individuals, families, and communities. Pennsylvania, with its mix of urban, suburban, and \n",
      "rural regions, faces a diverse array of emergencies ranging from floods and severe storms to \n",
      "public health crises and housing instability. To ensure an effective res\n",
      "\n",
      "--- 211 RESPONDS TO URGENT NEEDS.pdf ---\n",
      "211 RESPONDS TO URGENT NEEDS \n",
      "FACT\n",
      "211 stood up a statewide text\n",
      "response to support employees\n",
      "impacted by the partial federal\n",
      "government shutdown who did\n",
      "not know when they would\n",
      "receive their next paycheck.\n",
      "211 assists in times of\n",
      "disaster and widespread\n",
      "need\n",
      "FACT\n",
      "FACT\n",
      "1\n",
      "PLEASE VOTE TO INCLUDE FUNDING FOR PENNSYLVANIA'S 211 SYSTEM IN THE STATE BUDGET TO\n",
      "SUPPORT 211'S CAPACITY TO HELP OUR COMMUNITIES IN TIMES OF DISASTER OR GREAT NEED.\n",
      "1 2-1-1 Data\n",
      "In January 2019, 187 individuals\n",
      "subscribed to\n",
      "\n",
      "--- PEMA.pdf ---\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESS\n",
      "GUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready.PA.gov \n",
      "readypa@pa.gov\n",
      "\n",
      "Emergency Preparedness Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page\n",
      "\n",
      "--- ready-gov_disaster-preparedness-guide-for-older-adults.pdf ---\n",
      "1\n",
      "TAKE  \n",
      "CONTROL IN\n",
      "1\n",
      "2\n",
      "3\n",
      "Disaster Preparedness Guide for Older Adults\n",
      "\n",
      " \n",
      " \n",
      "STEP 1 | ASSESS YOUR NEEDS\n",
      "First, know your risk. Then, understand your needs during emergencies. \n",
      "This section guides you through a self-assessment process to identify your \n",
      "specific needs so that you can create a personalized emergency plan.\n",
      "STEP 2 | MAKE A PLAN\n",
      "Develop a comprehensive emergency plan and emergency \n",
      "preparedness kit tailored to your unique needs. This section ensures \n",
      "you are well prepared to respond to\n",
      "\n",
      "--- Substantial Damages Toolkit.pdf ---\n",
      " \n",
      " \n",
      " \n",
      "Prepared for: \n",
      "Pennsylvania Emergency Management \n",
      "Agency \n",
      "Emergency Management, Mitigation, \n",
      "Insurance, and Resilient Communities \n",
      "(MIRC) Office \n",
      "1310 Elmerton Avenue  \n",
      "Harrisburg, Pennsylvania 17110 \n",
      "Prepared by: \n",
      "PG Environmental and ERG \n",
      "14555 Avion Parkway, Suite 125 \n",
      "Chantilly, VA 20151 \n",
      " \n",
      "Substantial Improvements /  \n",
      "Substantial Damages Toolkit \n",
      "APRIL 2023 \n",
      "\n",
      " \n",
      "ii \n",
      " \n",
      "Contents \n",
      "Introduction ................................................................................................\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_texts_from_folder(folder_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in a folder (recursively).\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDFs.\n",
    "    Returns:\n",
    "        dict: {pdf_filename: extracted_text, ...}\n",
    "    \"\"\"\n",
    "    pdf_texts = {}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    pdf_texts[pdf_path] = extract_text_from_pdf(pdf_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {pdf_path}: {e}\")\n",
    "    return pdf_texts\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs\"\n",
    "pdf_texts = extract_texts_from_folder(folder_path)\n",
    "\n",
    "for pdf_file, text in pdf_texts.items():\n",
    "    print(f\"\\n--- {os.path.basename(pdf_file)} ---\")\n",
    "    print(text[:500])  # Print the first 500 characters to verify extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation\n",
    "We'll create a basic vector store to manage document chunks and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/PA 211 Disaster Community Resources.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/211 RESPONDS TO URGENT NEEDS.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/PEMA.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/ready-gov_disaster-preparedness-guide-for-older-adults.pdf\n",
      "Processing PDF: /Users/kekunkoya/Desktop/RAG Google 2/PDFs/Substantial Damages Toolkit.pdf\n",
      "Vector store populated with Gemini embeddings from all PDFs.\n",
      "\n",
      "Searching for items similar to: 'how to make a plan for my family'\n",
      "\n",
      "Top 3 search results:\n",
      "  - Text: fore an emergency happens, sit down together and decide how you will get in  contact with each other, what mobility and/ or medication issues will nee...\n",
      "    Similarity: 0.6842\n",
      "    Metadata: {'source': 'PEMA.pdf', 'chunk_id': 65}\n",
      "  - Text:  your family and friends have a plan in case of an emergency. Fill  out these cards and give one to each of them to make sure they know who to call an...\n",
      "    Similarity: 0.6811\n",
      "    Metadata: {'source': 'PEMA.pdf', 'chunk_id': 67}\n",
      "  - Text: r needs, it is time to make a plan and  build a kit. Sometimes disasters strike with little to no warning, so it is  important to have a plan and be p...\n",
      "    Similarity: 0.6771\n",
      "    Metadata: {'source': 'ready-gov_disaster-preparedness-guide-for-older-adults.pdf', 'chunk_id': 5}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz  # pip install PyMuPDF\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []   # List to store embedding vectors\n",
    "        self.texts = []     # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "            norm_query = np.linalg.norm(query_vector)\n",
    "            norm_vector = np.linalg.norm(vector)\n",
    "            if norm_query == 0 or norm_vector == 0:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (norm_query * norm_vector)\n",
    "            similarities.append((i, similarity))\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        return results\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def create_gemini_embedding(text, model=\"models/embedding-001\"):\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return response['embedding']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load API key\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Gemini API configuration: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Create the vector store\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # Folder containing PDFs\n",
    "    folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs/\"\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing PDF: {pdf_path}\")\n",
    "                try:\n",
    "                    text = extract_text_from_pdf(pdf_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {pdf_path}: {e}\")\n",
    "                    continue\n",
    "                chunks = chunk_text(text, chunk_size=1000, overlap=200)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if not chunk.strip():\n",
    "                        continue\n",
    "                    try:\n",
    "                        embedding = create_gemini_embedding(chunk)\n",
    "                        store.add_item(chunk, embedding, metadata={\"source\": file, \"chunk_id\": i})\n",
    "                    except Exception as e:\n",
    "                        print(f\"Embedding failed for {file} chunk {i}: {e}\")\n",
    "\n",
    "    print(\"Vector store populated with Gemini embeddings from all PDFs.\")\n",
    "\n",
    "    # Sample similarity search\n",
    "    query_text = \"how to make a plan for my family\"\n",
    "    print(f\"\\nSearching for items similar to: '{query_text}'\")\n",
    "    query_embedding = create_gemini_embedding(query_text)\n",
    "    search_results = store.similarity_search(query_embedding, k=3)\n",
    "\n",
    "    print(\"\\nTop 3 search results:\")\n",
    "    for result in search_results:\n",
    "        print(f\"  - Text: {result['text'][:150].replace('\\n',' ')}...\")\n",
    "        print(f\"    Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"    Metadata: {result['metadata']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called and 'client' is not used.\n",
    "\n",
    "def create_embeddings(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified Gemini model.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings. Defaults to \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    # Gemini's embed_content can handle both a single string or a list of strings\n",
    "    # in the 'content' parameter.\n",
    "    response = genai.embed_content(\n",
    "        model=model,\n",
    "        content=text\n",
    "    )\n",
    "\n",
    "    # If the original input was a single string, return just the first embedding vector.\n",
    "    if isinstance(text, str):\n",
    "        return response['embedding']\n",
    "\n",
    "    # Otherwise, return all embedding vectors as a list of lists.\n",
    "    return response['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_folder(folder_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process all PDFs in a folder for use with adaptive retrieval.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDF files.\n",
    "        chunk_size (int): Size of each chunk in characters.\n",
    "        chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore]: All document chunks and combined vector store.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"\\nExtracting text from PDF: {pdf_path}\")\n",
    "                extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "                if not extracted_text:\n",
    "                    print(f\"Failed to extract text from {pdf_path}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                print(\"Chunking text...\")\n",
    "                chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "                print(f\"Created {len(chunks)} text chunks for {file}\")\n",
    "\n",
    "                print(\"Creating embeddings for chunks...\")\n",
    "                chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "                if len(chunks) != len(chunk_embeddings):\n",
    "                    print(f\"Error: Mismatch between number of chunks and embeddings for {file}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "                    store.add_item(\n",
    "                        text=chunk,\n",
    "                        embedding=embedding,\n",
    "                        metadata={\"index\": i, \"source\": file}\n",
    "                    )\n",
    "                all_chunks.extend(chunks)\n",
    "                print(f\"Added {len(chunks)} chunks from {file} to the vector store.\")\n",
    "\n",
    "    print(f\"\\nAll done! Processed {len(all_chunks)} chunks from all PDFs.\")\n",
    "    return all_chunks, store\n",
    "\n",
    "# Example usage:\n",
    "# folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs/\"\n",
    "# all_chunks, store = process_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def classify_query(query, model=\"gemini-pro\"):\n",
    "    \"\"\"\n",
    "    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        model (str): LLM model to use. Defaults to \"gemini-2.0-flash\".\n",
    "        \n",
    "    Returns:\n",
    "        str: Query category\n",
    "    \"\"\"\n",
    "    # Define the prompt to guide the AI's classification\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at classifying questions.\n",
    "    Classify the given query into exactly one of these categories:\n",
    "    - Factual: Queries seeking specific, verifiable information.\n",
    "    - Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "    - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "    - Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "    Return ONLY the category name, without any explanation or additional text.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Category:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate the classification response from the AI model\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.0, # Low temperature for deterministic output\n",
    "                max_output_tokens=20 # Limit output to ensure it's just the category name\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and strip the category from the response\n",
    "        category = response.text.strip()\n",
    "    \n",
    "        # Define the list of valid categories\n",
    "        valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "        \n",
    "        # Ensure the returned category is a valid, single word\n",
    "        for valid in valid_categories:\n",
    "            if valid.lower() in category.lower():\n",
    "                return valid\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during query classification: {e}\")\n",
    "        # Default to \"Factual\" if classification fails\n",
    "        return \"Factual\"\n",
    "    \n",
    "    # Default to \"Factual\" if classification is not one of the valid categories\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Specialized Retrieval Strategies\n",
    "### 1. Factual Strategy - Focus on Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def call_gemini(prompt, model=\"gemini-2.0-flash\", temperature=0):\n",
    "    \"\"\"A helper function to make a call to the Gemini API.\"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for factual queries focusing on precision.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Use LLM to enhance the query for better precision\n",
    "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
    "    Your task is to reformulate the given factual query to make it more precise and\n",
    "    specific for information retrieval. Focus on key entities and their relationships.\n",
    "    Provide ONLY the enhanced query without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Enhance this factual query: {query}\"\n",
    "    \n",
    "    # Generate the enhanced query using the LLM\n",
    "    enhanced_query_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    enhanced_query = call_gemini(enhanced_query_prompt, model=\"gemini-2.0-flash\", temperature=0)\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    # Create embeddings for the enhanced query\n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "    \n",
    "    # Perform initial similarity search to retrieve documents\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Initialize a list to store ranked results\n",
    "    ranked_results = []\n",
    "    \n",
    "    # Score and rank documents by relevance using LLM\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # Sort the results by relevance score in descending order\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return ranked_results[:k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analytical Strategy - Comprehensive Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for analytical queries focusing on comprehensive coverage.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "\n",
    "    # Define the prompt to guide the AI in generating sub-questions\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at breaking down complex questions.\n",
    "    Generate sub-questions that explore different aspects of the main analytical query.\n",
    "    These sub-questions should cover the breadth of the topic and help retrieve\n",
    "    comprehensive information.\n",
    "\n",
    "    Return a list of exactly 3 sub-questions, one per line.\n",
    "\n",
    "    Main query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    # Generate the sub-questions using the LLM\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=150 # A reasonable limit for 3 questions\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the sub-questions\n",
    "        sub_queries = response.text.strip().split('\\n')\n",
    "        sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "        print(f\"Generated sub-queries: {sub_queries}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during sub-query generation: {e}\")\n",
    "        sub_queries = [query] # Fallback to the original query\n",
    "    \n",
    "    # Retrieve documents for each sub-query\n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        # Create embeddings for the sub-query\n",
    "        sub_query_embedding = create_embeddings(sub_query)\n",
    "        # Perform similarity search for the sub-query\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Ensure diversity by selecting from different sub-query results\n",
    "    # Remove duplicates (same text content)\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "    \n",
    "    # If we need more results to reach k, add more from initial results\n",
    "    if len(diverse_results) < k:\n",
    "        # Direct retrieval for the main query\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "    \n",
    "    # Return the top k diverse results\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opinion Strategy - Diverse Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "# Also assume that `create_embeddings` and `SimpleVectorStore` are defined.\n",
    "\n",
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for opinion queries focusing on diverse perspectives.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Define the prompt to guide the AI in identifying different perspectives\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at identifying different perspectives on a topic.\n",
    "    For the given query about opinions or viewpoints, identify different perspectives\n",
    "    that people might have on this topic.\n",
    "\n",
    "    Return a list of exactly 3 different viewpoint angles, one per line.\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    \n",
    "    # Generate the different perspectives using the LLM\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=150 # A reasonable limit for 3 short viewpoints\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract and clean the viewpoints\n",
    "        viewpoints = response.text.strip().split('\\n')\n",
    "        viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "        print(f\"Identified viewpoints: {viewpoints}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during viewpoint generation: {e}\")\n",
    "        # Fallback to a simple retrieval if viewpoint generation fails\n",
    "        viewpoint_embedding = create_embeddings(query)\n",
    "        return vector_store.similarity_search(viewpoint_embedding, k=k)\n",
    "    \n",
    "    # Retrieve documents representing each viewpoint\n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Combine the main query with the viewpoint\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "        # Create embeddings for the combined query\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "        # Perform similarity search for the combined query\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        # Mark results with the viewpoint they represent\n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "        \n",
    "        # Add the results to the list of all results\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Select a diverse range of opinions\n",
    "    # Ensure we get at least one document from each viewpoint if possible\n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Filter documents by viewpoint\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    # Fill remaining slots with highest similarity docs\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        # Sort remaining docs by similarity\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "    \n",
    "    # Return the top k results\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contextual Strategy - User Context Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def call_gemini(prompt, model=\"gemini-pro\", temperature=0):\n",
    "    \"\"\"A helper function to make a call to the Gemini API.\"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for contextual queries integrating user context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        user_context (str): Additional user context\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # If no user context provided, try to infer it from the query\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
    "        For the given query, infer what contextual information might be relevant or implied\n",
    "        but not explicitly stated. Focus on what background would help answering this query.\n",
    "\n",
    "        Return a brief description of the implied context.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
    "        \n",
    "        inferred_context_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "        \n",
    "        try:\n",
    "            # Generate the inferred context using the LLM\n",
    "            user_context = call_gemini(inferred_context_prompt, model=\"gemini-2.0-flash\", temperature=0.1)\n",
    "            print(f\"Inferred context: {user_context}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inferring context: {e}\")\n",
    "            user_context = \"\" # Fallback to empty context\n",
    "    \n",
    "    # Reformulate the query to incorporate context\n",
    "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
    "    Given a query and some contextual information, create a more specific query that\n",
    "    incorporates the context to get more relevant information.\n",
    "\n",
    "    Return ONLY the reformulated query without explanation.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    Reformulate the query to incorporate this context:\"\"\"\n",
    "    \n",
    "    contextualized_query_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    \n",
    "    try:\n",
    "        # Generate the contextualized query using the LLM\n",
    "        contextualized_query = call_gemini(contextualized_query_prompt, model=\"gemini-2.0-flash\", temperature=0)\n",
    "        print(f\"Contextualized query: {contextualized_query}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reformulating query: {e}\")\n",
    "        contextualized_query = query # Fallback to original query\n",
    "    \n",
    "    # Retrieve documents based on the contextualized query\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Rank documents considering both relevance and user context\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        # Score document relevance considering the context\n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    # Sort by context relevance and return top k results\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Document Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import re\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def score_document_relevance(query, document, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Score document relevance to a query using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        document (str): Document text\n",
    "        model (str): LLM model. Defaults to \"gemini-2.0-flash\".\n",
    "\n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
    "    Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
    "    0 = Completely irrelevant\n",
    "    10 = Perfectly addresses the query\n",
    "\n",
    "    Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    # Gemini models have higher context limits, but truncating is still good practice.\n",
    "    doc_preview = document[:4000] + \"...\" if len(document) > 4000 else document\n",
    "\n",
    "    # User prompt containing the query and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine the system and user prompts into a single prompt for Gemini\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    # Create a GenerativeModel instance\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate response from the model\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.0, # Low temperature for a deterministic score\n",
    "                max_output_tokens=10 # Keep output short\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Extract the score from the model's response\n",
    "        score_text = response.text.strip()\n",
    "        \n",
    "        # Extract numeric score using regex\n",
    "        match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "        if match:\n",
    "            score = float(match.group(1))\n",
    "            return min(10.0, max(0.0, score))  # Ensure score is within 0-10\n",
    "        else:\n",
    "            # Default score if extraction fails\n",
    "            return 5.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during relevance scoring: {e}\")\n",
    "        return 5.0 # Return a neutral score on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Adaptive Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Perform adaptive retrieval by selecting and executing the appropriate strategy.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context for contextual queries\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    # Classify the query to determine its type\n",
    "    try:\n",
    "        query_type = classify_query(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying query. Falling back to Factual retrieval. Details: {e}\")\n",
    "        query_type = \"Factual\"\n",
    "        \n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "\n",
    "    # Select and execute the appropriate retrieval strategy based on the query type\n",
    "    if query_type == \"Factual\":\n",
    "        # Use the factual retrieval strategy for precise information\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # Use the analytical retrieval strategy for comprehensive coverage\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # Use the opinion retrieval strategy for diverse perspectives\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # Use the contextual retrieval strategy, incorporating user context\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # Default to factual retrieval strategy if classification fails\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "        \n",
    "    return results  # Return the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Generation\n",
    "# \n",
    "# import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def generate_response(query, results, query_type, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on query, retrieved documents, and query type.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved documents\n",
    "        query_type (str): Type of query\n",
    "        model (str): LLM model. Defaults to \"gemini-2.0-flash\".\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Prepare context from retrieved documents by joining their texts with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "\n",
    "    # Create custom system prompt based on query type\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
    "        Answer the question based on the provided context. Focus on accuracy and precision.\n",
    "        If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
    "        Based on the provided context, offer a comprehensive analysis of the topic.\n",
    "        Cover different aspects and perspectives in your explanation.\n",
    "        If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
    "\n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
    "        Based on the provided context, present different perspectives on the topic.\n",
    "        Ensure fair representation of diverse opinions without showing bias.\n",
    "        Acknowledge where the context presents limited viewpoints.\"\"\"\n",
    "\n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
    "        Answer the question considering both the query and its context.\n",
    "        Make connections between the query context and the information in the provided documents.\n",
    "        If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
    "\n",
    "    # Create a single user prompt by combining the system prompt, context, and query\n",
    "    user_prompt = f\"\"\"\n",
    "    {system_prompt}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a helpful response based on the context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Gemini GenerativeModel\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate response using the Gemini API\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.2 # Temperature for some creativity\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Return the generated response content\n",
    "        return response.text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while generating the response: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Adaptive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with adaptive retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, retrieved documents, query type, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # Generate a response based on the query, retrieved documents, and query type\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # Compile the results into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai python-dotenv pymupdf\n",
    "import os, time, json\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -------------------- Gemini setup --------------------\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set in your environment.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "GEN_MODEL = \"gemini-2.0-flash\"\n",
    "EMBED_MODEL = \"models/embedding-001\"\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def _gen_with_retry(prompt, temperature=0, max_retries=5, base_sleep=5):\n",
    "    model = genai.GenerativeModel(GEN_MODEL)\n",
    "    for att in range(max_retries):\n",
    "        try:\n",
    "            resp = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "            )\n",
    "            return (resp.text or \"\").strip()\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
    "                sleep_s = base_sleep * (att+1)\n",
    "                print(f\"[generate] Quota hit. Sleeping {sleep_s}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "def _embed_one(text, max_retries=5, base_sleep=5):\n",
    "    for att in range(max_retries):\n",
    "        try:\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=text)\n",
    "            # normalize response\n",
    "            if isinstance(resp, dict):\n",
    "                emb = resp.get(\"embedding\", {}).get(\"values\")\n",
    "                if emb is None and \"embedding\" in resp:\n",
    "                    emb = resp[\"embedding\"]\n",
    "            else:\n",
    "                emb = getattr(resp, \"embedding\", None)\n",
    "                emb = emb.values if hasattr(emb, \"values\") else emb\n",
    "            if emb is None:\n",
    "                raise RuntimeError(f\"Unexpected embedding response: {type(resp)} {resp}\")\n",
    "            arr = np.array(emb, dtype=np.float32)\n",
    "            return arr if arr.ndim == 1 else arr.reshape(-1)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
    "                sleep_s = base_sleep * (att+1)\n",
    "                print(f\"[embed] Quota hit. Sleeping {sleep_s}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "def cosine(a, b):\n",
    "    an = np.linalg.norm(a); bn = np.linalg.norm(b)\n",
    "    if an == 0 or bn == 0: return 0.0\n",
    "    return float(np.dot(a, b) / (an * bn))\n",
    "\n",
    "# -------------------- Minimal vector store (NumPy) --------------------\n",
    "class GeminiVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding, dtype=np.float32))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "\n",
    "    def add_texts(self, texts, metadatas=None, sleep_between=0.0):\n",
    "        metadatas = metadatas or [{}] * len(texts)\n",
    "        for t, m in zip(texts, metadatas):\n",
    "            self.add_item(t, _embed_one(t), m)\n",
    "            if sleep_between: time.sleep(sleep_between)\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=4, filter_func=None):\n",
    "        if not self.vectors: return []\n",
    "        q = np.array(query_embedding, dtype=np.float32)\n",
    "        qn = np.linalg.norm(q)\n",
    "        sims = []\n",
    "        for i, v in enumerate(self.vectors):\n",
    "            if filter_func and not filter_func(self.metadata[i]): \n",
    "                continue\n",
    "            vn = np.linalg.norm(v)\n",
    "            sim = 0.0 if qn == 0 or vn == 0 else float(np.dot(q, v) / (qn * vn))\n",
    "            sims.append((i, sim))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        out = []\n",
    "        for idx, score in sims[:k]:\n",
    "            out.append({\"text\": self.texts[idx], \"metadata\": self.metadata[idx], \"similarity\": score})\n",
    "        return out\n",
    "\n",
    "    def similarity_search_by_text(self, query_text, k=4, filter_func=None):\n",
    "        q_emb = _embed_one(query_text)\n",
    "        return self.similarity_search(q_emb, k=k, filter_func=filter_func)\n",
    "\n",
    "# -------------------- PDF -> chunks -> store --------------------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        if end == len(text): break\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def process_document_gemini(pdf_path, chunk_size=1000, overlap=200):\n",
    "    raw = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(raw, chunk_size, overlap)\n",
    "    vs = GeminiVectorStore()\n",
    "    vs.add_texts(chunks)  # embeds each chunk\n",
    "    return chunks, vs\n",
    "\n",
    "# -------------------- Query classification & retrieval --------------------\n",
    "def classify_query_gemini(query):\n",
    "    # Cheap heuristic first; adjust as you like\n",
    "    q = query.lower()\n",
    "    if any(x in q for x in [\"why\", \"how\", \"analyz\", \"compare\", \"tradeoff\"]): return \"Analytical\"\n",
    "    if any(x in q for x in [\"opinion\", \"should i\", \"what do you think\"]): return \"Opinion\"\n",
    "    if any(x in q for x in [\"context\", \"background\", \"overview\", \"explain\"]): return \"Contextual\"\n",
    "    return \"Factual\"\n",
    "\n",
    "def adaptive_retrieval_gemini(query, vector_store, k=4):\n",
    "    qtype = classify_query_gemini(query)\n",
    "    # simple policy: factual/contextual => higher k, analytical => same k, opinion => smaller k\n",
    "    kk = {\"Factual\": max(4, k), \"Contextual\": max(5, k), \"Analytical\": k, \"Opinion\": max(3, k//1)}.get(qtype, k)\n",
    "    docs = vector_store.similarity_search_by_text(query, k=kk)\n",
    "    return qtype, docs\n",
    "\n",
    "# -------------------- Response generator --------------------\n",
    "def generate_response_gemini(query, docs, query_type=\"General\"):\n",
    "    context = \"\\n\\n\".join([f\"[{i+1}] {d['text']}\" for i, d in enumerate(docs)])\n",
    "    prompt = f\"\"\"You are a helpful assistant answering with ONLY the provided context if possible.\n",
    "Query type: {query_type}\n",
    "User query: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Cite supporting chunk indices like [1], [2] in-line.\n",
    "- If the answer is not in context, say so briefly and provide best-effort guidance.\n",
    "- Be concise (120-180 words).\n",
    "\"\"\"\n",
    "    return _gen_with_retry(prompt, temperature=0)\n",
    "\n",
    "# -------------------- Optional comparison against references --------------------\n",
    "def compare_responses(results):\n",
    "    # Simple embedding-based score vs reference, averaged\n",
    "    scores = []\n",
    "    for r in results:\n",
    "        ref = r.get(\"reference_answer\")\n",
    "        if not ref: \n",
    "            continue\n",
    "        # score standard\n",
    "        std_emb = _embed_one(r[\"standard_retrieval\"][\"response\"])\n",
    "        adp_emb = _embed_one(r[\"adaptive_retrieval\"][\"response\"])\n",
    "        ref_emb = _embed_one(ref)\n",
    "        scores.append({\n",
    "            \"std\": cosine(std_emb, ref_emb),\n",
    "            \"adp\": cosine(adp_emb, ref_emb)\n",
    "        })\n",
    "    if not scores:\n",
    "        return {\"message\": \"No references provided; skipping quantitative comparison.\"}\n",
    "    std_avg = float(np.mean([s[\"std\"] for s in scores]))\n",
    "    adp_avg = float(np.mean([s[\"adp\"] for s in scores]))\n",
    "    return {\n",
    "        \"standard_avg_sim\": round(std_avg, 3),\n",
    "        \"adaptive_avg_sim\": round(adp_avg, 3),\n",
    "        \"winner\": \"adaptive\" if adp_avg > std_avg else (\"standard\" if std_avg > adp_avg else \"tie\")\n",
    "    }\n",
    "\n",
    "# -------------------- The function you asked to convert --------------------\n",
    "def evaluate_adaptive_vs_standard_gemini(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Gemini-powered evaluation of adaptive vs standard retrieval.\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL (Gemini) ===\")\n",
    "\n",
    "    # Build chunks & vector store\n",
    "    chunks, vector_store = process_document_gemini(pdf_path)\n",
    "\n",
    "    results = []\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
    "\n",
    "        # --- Standard retrieval ---\n",
    "        print(\"\\n--- Standard Retrieval ---\")\n",
    "        query_embedding = _embed_one(query)\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        standard_response = generate_response_gemini(query, standard_docs, \"General\")\n",
    "\n",
    "        # --- Adaptive retrieval ---\n",
    "        print(\"\\n--- Adaptive Retrieval ---\")\n",
    "        qtype, adaptive_docs = adaptive_retrieval_gemini(query, vector_store, k=4)\n",
    "        adaptive_response = generate_response_gemini(query, adaptive_docs, qtype)\n",
    "\n",
    "        entry = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": qtype,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            entry[\"reference_answer\"] = reference_answers[i]\n",
    "\n",
    "        results.append(entry)\n",
    "\n",
    "        print(\"\\n--- Responses ---\")\n",
    "        print(f\"Standard: {standard_response[:200]}...\")\n",
    "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
    "\n",
    "    comparison = None\n",
    "    if reference_answers:\n",
    "        comparison = compare_responses(results)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(comparison)\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai python-dotenv\n",
    "import os, time\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- one-time setup (safe to move elsewhere) ---\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "_GEMINI_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "def _gen_with_retry(system_prompt: str, user_prompt: str, temperature: float = 0.2,\n",
    "                    max_retries: int = 5, base_sleep: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Generate with Gemini, retrying on quota (429/ResourceExhausted).\n",
    "    Returns response text ('' if unavailable).\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(_GEMINI_MODEL)\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = model.generate_content(\n",
    "                contents=[\n",
    "                    {\"role\": \"system\", \"parts\": [system_prompt]},\n",
    "                    {\"role\": \"user\", \"parts\": [user_prompt]},\n",
    "                ],\n",
    "                generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "            )\n",
    "            return (resp.text or \"\").strip()\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
    "                sleep_s = base_sleep * (attempt + 1)\n",
    "                print(f\"[Gemini] Quota hit (attempt {attempt+1}/{max_retries}). Sleeping {sleep_s}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            raise\n",
    "    return \"\"\n",
    "\n",
    "def compare_responses_gemini(results):\n",
    "    \"\"\"\n",
    "    Compare standard and adaptive responses against reference answers using Gemini.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): items with keys:\n",
    "          - 'query', 'query_type'\n",
    "          - 'reference_answer' (optional)\n",
    "          - 'standard_retrieval': {'response': str}\n",
    "          - 'adaptive_retrieval': {'response': str}\n",
    "    Returns:\n",
    "        str: Markdown comparison analysis.\n",
    "    \"\"\"\n",
    "    comparison_prompt = (\n",
    "        \"You are an expert evaluator of information retrieval systems. \"\n",
    "        \"Compare the standard retrieval and adaptive retrieval responses for each query. \"\n",
    "        \"Assess accuracy, relevance, comprehensiveness, grounding in the provided reference, \"\n",
    "        \"and hallucination risk. Be concise but specific. End with a 12 sentence verdict.\"\n",
    "    )\n",
    "\n",
    "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        if \"reference_answer\" not in result:\n",
    "            # Skip queries with no reference\n",
    "            continue\n",
    "\n",
    "        query = result.get(\"query\", \"\")\n",
    "        qtype = result.get(\"query_type\", \"Unknown\")\n",
    "        ref = result.get(\"reference_answer\", \"\")\n",
    "        std_resp = result.get(\"standard_retrieval\", {}).get(\"response\", \"\")\n",
    "        adp_resp = result.get(\"adaptive_retrieval\", {}).get(\"response\", \"\")\n",
    "\n",
    "        comparison_text += f\"## Query {i+1}: {query}\\n\"\n",
    "        comparison_text += f\"*Query Type: {qtype}*\\n\\n\"\n",
    "        comparison_text += f\"**Reference Answer:**\\n{ref}\\n\\n\"\n",
    "        comparison_text += f\"**Standard Retrieval Response:**\\n{std_resp}\\n\\n\"\n",
    "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{adp_resp}\\n\\n\"\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"Reference Answer:\\n{ref}\\n\\n\"\n",
    "            f\"Standard Retrieval Response:\\n{std_resp}\\n\\n\"\n",
    "            f\"Adaptive Retrieval Response:\\n{adp_resp}\\n\\n\"\n",
    "            \"Provide a structured comparison:\\n\"\n",
    "            \"1) Accuracy vs reference\\n\"\n",
    "            \"2) Relevance / coverage of key points\\n\"\n",
    "            \"3) Hallucinations / unsupported claims\\n\"\n",
    "            \"4) Clarity and organization\\n\"\n",
    "            \"Finish with: Verdict - <which is better and why>.\"\n",
    "        )\n",
    "\n",
    "        analysis = _gen_with_retry(\n",
    "            system_prompt=comparison_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            temperature=0.2\n",
    "        ) or \"_(No analysis returned)_\"\n",
    "\n",
    "        comparison_text += f\"**Comparison Analysis:**\\n{analysis}\\n\\n\"\n",
    "\n",
    "    return comparison_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Adaptive Retrieval System (Customized Queries)\n",
    "\n",
    "The final step to use the adaptive RAG evaluation system is to call the evaluate_adaptive_vs_standard() function with your PDF document and test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_first_embedding(resp):\n",
    "    \"\"\"\n",
    "    Normalize Gemini embed_content responses to a single 1-D list of floats.\n",
    "    Handles dict/object/list variants across SDK versions.\n",
    "    \"\"\"\n",
    "    # 1) Dict responses\n",
    "    if isinstance(resp, dict):\n",
    "        # Single embedding\n",
    "        if \"embedding\" in resp:\n",
    "            emb = resp[\"embedding\"]\n",
    "            if isinstance(emb, dict) and \"values\" in emb:\n",
    "                return emb[\"values\"]\n",
    "            return emb  # already a list of floats\n",
    "        # Batch embeddings\n",
    "        if \"embeddings\" in resp and resp[\"embeddings\"]:\n",
    "            emb0 = resp[\"embeddings\"][0]\n",
    "            if isinstance(emb0, dict) and \"values\" in emb0:\n",
    "                return emb0[\"values\"]\n",
    "            return emb0  # already list\n",
    "        raise RuntimeError(f\"Unexpected dict embed response: keys={list(resp.keys())}\")\n",
    "\n",
    "    # 2) Object-like responses\n",
    "    if hasattr(resp, \"embedding\") and resp.embedding is not None:\n",
    "        emb = resp.embedding\n",
    "        return getattr(emb, \"values\", emb)\n",
    "    if hasattr(resp, \"embeddings\") and resp.embeddings:\n",
    "        emb0 = resp.embeddings[0]\n",
    "        return getattr(emb0, \"values\", emb0)\n",
    "\n",
    "    # 3) List responses (SDK sometimes returns a list)\n",
    "    if isinstance(resp, list):\n",
    "        if not resp:\n",
    "            raise RuntimeError(\"Empty list embed response.\")\n",
    "        # If it's already a flat list of numbers\n",
    "        if all(isinstance(x, (int, float)) for x in resp):\n",
    "            return resp\n",
    "        # If it's a list of dicts/objects, take the first\n",
    "        first = resp[0]\n",
    "        if isinstance(first, dict) and \"values\" in first:\n",
    "            return first[\"values\"]\n",
    "        if hasattr(first, \"values\"):\n",
    "            return first.values\n",
    "        # If the first is a nested list of floats\n",
    "        if isinstance(first, list) and all(isinstance(x, (int, float)) for x in first):\n",
    "            return first\n",
    "        raise RuntimeError(f\"Unexpected list embed response element type: {type(first)}\")\n",
    "\n",
    "    raise RuntimeError(f\"Unknown embed response type: {type(resp)}\")\n",
    "\n",
    "\n",
    "def _to_1d(vec) -> np.ndarray:\n",
    "    arr = np.array(vec, dtype=np.float32)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        # Take first row if a batch slips through\n",
    "        return arr[0]\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "\n",
    "def _embed_one(text: str, max_retries: int = 5, base_sleep: float = 5.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed a single string using Gemini. Returns a 1-D np.ndarray.\n",
    "    Retries on 429/ResourceExhausted.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=text)\n",
    "            emb = _extract_first_embedding(resp)\n",
    "            return _to_1d(emb)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
    "                sleep_s = base_sleep * (attempt + 1)\n",
    "                print(f\"[embed] Quota hit (attempt {attempt+1}/{max_retries}). Sleeping {sleep_s}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            # Helpful debug for format surprises\n",
    "            print(f\"[embed] Unexpected response type: {type(resp) if 'resp' in locals() else None}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDFs in /Users/kekunkoya/Desktop/RAG Google 2/PDFs:\n",
      " - PA 211 Disaster Community Resources.pdf\n",
      " - 211 RESPONDS TO URGENT NEEDS.pdf\n",
      " - PEMA.pdf\n",
      " - ready-gov_disaster-preparedness-guide-for-older-adults.pdf\n",
      " - Substantial Damages Toolkit.pdf\n"
     ]
    }
   ],
   "source": [
    "# pip install google-generativeai python-dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---- Load environment and configure Gemini ----\n",
    "load_dotenv()\n",
    "import google.generativeai as genai\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set in .env or environment variables.\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# ---- Path to your knowledge source folder (containing all PDFs) ----\n",
    "pdf_folder = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs\"\n",
    "\n",
    "# ---- Build a list of all PDF paths ----\n",
    "pdf_paths = [\n",
    "    os.path.join(pdf_folder, filename)\n",
    "    for filename in os.listdir(pdf_folder)\n",
    "    if filename.lower().endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "# ---- Debug: print what we found ----\n",
    "if not pdf_paths:\n",
    "    print(f\"No PDFs found in folder: {pdf_folder}\")\n",
    "else:\n",
    "    print(f\"Found {len(pdf_paths)} PDFs in {pdf_folder}:\")\n",
    "    for path in pdf_paths:\n",
    "        print(f\" - {os.path.basename(path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard_gemini(pdf_paths, test_queries, reference_answers=None,\n",
    "                                         chunk_size=1000, overlap=200, k=4):\n",
    "    \"\"\"\n",
    "    Gemini-powered evaluation that accepts a list of PDF paths.\n",
    "    - Concats all PDF text,\n",
    "    - Chunks + embeds with Gemini,\n",
    "    - Compares standard vs. adaptive retrieval for each query.\n",
    "\n",
    "    Requires:\n",
    "      - extract_text_from_pdf(path) -> str\n",
    "      - chunk_text(text, chunk_size, overlap) -> List[str]\n",
    "      - class GeminiVectorStore with .add_texts([...]) and .similarity_search_by_text(query, k)\n",
    "      - adaptive_retrieval_gemini(query, vector_store, k) -> (query_type, docs)\n",
    "      - generate_response_gemini(query, docs, query_type) -> str\n",
    "      - (optional) compare_responses_gemini(results) -> str\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL (Gemini, Multi-PDF) ===\")\n",
    "\n",
    "    # 1) Combine all PDF text\n",
    "    all_text = []\n",
    "    for p in pdf_paths:\n",
    "        try:\n",
    "            all_text.append(extract_text_from_pdf(p))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p} due to extraction error: {e}\")\n",
    "    all_text = \"\\n\".join(t for t in all_text if t and t.strip())\n",
    "\n",
    "    if not all_text.strip():\n",
    "        raise ValueError(\"No extractable text found across provided PDFs.\")\n",
    "\n",
    "    # 2) Chunk + build vector store (Gemini embeddings under the hood)\n",
    "    chunks = chunk_text(all_text, chunk_size, overlap)\n",
    "    vector_store = GeminiVectorStore()\n",
    "    vector_store.add_texts(chunks)  # embeds each chunk via Gemini\n",
    "\n",
    "    # 3) Evaluate per query\n",
    "    results = []\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
    "\n",
    "        # --- Standard ---\n",
    "        print(\"\\n--- Standard Retrieval ---\")\n",
    "        std_docs = vector_store.similarity_search_by_text(query, k=k)\n",
    "        std_resp = generate_response_gemini(query, std_docs, \"General\")\n",
    "\n",
    "        # --- Adaptive ---\n",
    "        print(\"\\n--- Adaptive Retrieval ---\")\n",
    "        qtype, adp_docs = adaptive_retrieval_gemini(query, vector_store, k=k)\n",
    "        adp_resp = generate_response_gemini(query, adp_docs, qtype)\n",
    "\n",
    "        entry = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": qtype,\n",
    "            \"standard_retrieval\": {\"documents\": std_docs, \"response\": std_resp},\n",
    "            \"adaptive_retrieval\": {\"documents\": adp_docs, \"response\": adp_resp},\n",
    "        }\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            entry[\"reference_answer\"] = reference_answers[i]\n",
    "\n",
    "        results.append(entry)\n",
    "\n",
    "        print(\"\\n--- Responses ---\")\n",
    "        print(f\"Standard: {std_resp[:200]}...\")\n",
    "        print(f\"Adaptive:  {adp_resp[:200]}...\")\n",
    "\n",
    "    # 4) Optional comparison against references\n",
    "    comparison = None\n",
    "    if reference_answers:\n",
    "        try:\n",
    "            comparison = compare_responses_gemini(results)\n",
    "            print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "            print(comparison[:500] + (\"...\" if len(comparison) > 500 else \"\"))\n",
    "        except NameError:\n",
    "            # compare_responses_gemini not provided; skip gracefully\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison or \"No reference-based comparison (none provided or function missing).\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Results ---\n",
      "{'total_text_length': 326826, 'number_of_pdfs': 5, 'test_queries': ['What should I do if I am unable to evacuate during an emergency?'], 'reference_answers': ['Shelter in place in the safest room in your home, and inform your support network of your situation. Keep a radio or phone handy for updates from officials.']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def evaluate_adaptive_vs_standard(pdf_paths, test_queries, reference_answers):\n",
    "    \"\"\"\n",
    "    Accepts a list of PDF paths instead of a single path.\n",
    "    Returns a dictionary with a 'comparison' key for printout.\n",
    "    \"\"\"\n",
    "    # Combine all text from all PDFs\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_paths:\n",
    "        all_text += extract_text_from_pdf(pdf_path) + \"\\n\"\n",
    "    \n",
    "    # (Example placeholder logic, replace with your real chunking/retrieval/eval code)\n",
    "    # For now, let's just compare the length of the text for demonstration:\n",
    "    comparison = {\n",
    "        \"total_text_length\": len(all_text),\n",
    "        \"number_of_pdfs\": len(pdf_paths),\n",
    "        \"test_queries\": test_queries,\n",
    "        \"reference_answers\": reference_answers\n",
    "    }\n",
    "    return {\"comparison\": comparison}\n",
    "\n",
    "# --- Find all PDFs in the folder ---\n",
    "pdf_folder = \"/Users/kekunkoya/Desktop/Revised RAG Project/PDFs\"\n",
    "pdf_paths = [\n",
    "    os.path.join(pdf_folder, fname)\n",
    "    for fname in os.listdir(pdf_folder)\n",
    "    if fname.lower().endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "# --- Example test queries and reference answers ---\n",
    "test_queries = [\n",
    "    \"What should I do if I am unable to evacuate during an emergency?\" \n",
    "]\n",
    "reference_answers = [\n",
    "    \"Shelter in place in the safest room in your home, and inform your support network of your situation. Keep a radio or phone handy for updates from officials.\"\n",
    "]\n",
    "\n",
    "# --- Run evaluation and print result ---\n",
    "evaluation_results = evaluate_adaptive_vs_standard(\n",
    "    pdf_paths=pdf_paths,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "print(\"\\n--- Comparison Results ---\")\n",
    "print(evaluation_results[\"comparison\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_first_embedding(resp):\n",
    "    \"\"\"\n",
    "    Normalize google.generativeai embed_content responses into a single 1-D list of floats.\n",
    "    Handles dict/object/list variants across SDK versions.\n",
    "    \"\"\"\n",
    "    # dict (single)\n",
    "    if isinstance(resp, dict) and \"embedding\" in resp:\n",
    "        emb = resp[\"embedding\"]\n",
    "        return emb[\"values\"] if isinstance(emb, dict) and \"values\" in emb else emb\n",
    "\n",
    "    # dict (batch)\n",
    "    if isinstance(resp, dict) and \"embeddings\" in resp:\n",
    "        emb0 = resp[\"embeddings\"][0]\n",
    "        return emb0[\"values\"] if isinstance(emb0, dict) and \"values\" in emb0 else emb0\n",
    "\n",
    "    # object (single)\n",
    "    if hasattr(resp, \"embedding\") and resp.embedding is not None:\n",
    "        return getattr(resp.embedding, \"values\", resp.embedding)\n",
    "\n",
    "    # object (batch)\n",
    "    if hasattr(resp, \"embeddings\") and resp.embeddings:\n",
    "        emb0 = resp.embeddings[0]\n",
    "        return getattr(emb0, \"values\", emb0)\n",
    "\n",
    "    # list (SDK sometimes returns a list)\n",
    "    if isinstance(resp, list):\n",
    "        if not resp:\n",
    "            raise RuntimeError(\"Empty list embed response.\")\n",
    "        first = resp[0]\n",
    "        if isinstance(first, dict) and \"values\" in first:\n",
    "            return first[\"values\"]\n",
    "        if hasattr(first, \"values\"):\n",
    "            return first.values\n",
    "        if isinstance(first, list) and all(isinstance(x, (int, float)) for x in first):\n",
    "            return first\n",
    "        # already a flat list?\n",
    "        if all(isinstance(x, (int, float)) for x in resp):\n",
    "            return resp\n",
    "\n",
    "    raise RuntimeError(f\"Unexpected embed response type/shape: {type(resp)} -> {resp!r}\")\n",
    "\n",
    "\n",
    "def _to_1d(vec):\n",
    "    import numpy as np\n",
    "    arr = np.array(vec, dtype=np.float32)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        return arr[0]  # pick first if a batch sneaks in\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "\n",
    "def _embed_one(text: str, max_retries: int = 5, base_sleep: float = 5.0):\n",
    "    \"\"\"\n",
    "    Embed a single string using Gemini. Returns a 1-D np.ndarray.\n",
    "    Retries on 429/ResourceExhausted. Robust to SDK response variants.\n",
    "    \"\"\"\n",
    "    import time, numpy as np, google.generativeai as genai\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=text)\n",
    "            emb = _extract_first_embedding(resp)\n",
    "            return _to_1d(emb)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
    "                sleep_s = base_sleep * (attempt + 1)\n",
    "                print(f\"[embed] Quota hit (attempt {attempt+1}/{max_retries}). Sleeping {sleep_s}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            # Debug aid if the format is odd\n",
    "            print(f\"[embed] Unexpected response type: {type(resp) if 'resp' in locals() else None}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DROP-IN REPLACEMENTS (put these above GeminiVectorStore) ---\n",
    "\n",
    "def _extract_first_embedding(resp):\n",
    "    \"\"\"\n",
    "    Normalize google.generativeai embed_content responses into a single 1-D list of floats.\n",
    "    Handles dict/object/list variants across SDK versions.\n",
    "    \"\"\"\n",
    "    # dict (single)\n",
    "    if isinstance(resp, dict) and \"embedding\" in resp:\n",
    "        emb = resp[\"embedding\"]\n",
    "        return emb[\"values\"] if isinstance(emb, dict) and \"values\" in emb else emb\n",
    "\n",
    "    # dict (batch)\n",
    "    if isinstance(resp, dict) and \"embeddings\" in resp and resp[\"embeddings\"]:\n",
    "        emb0 = resp[\"embeddings\"][0]\n",
    "        return emb0[\"values\"] if isinstance(emb0, dict) and \"values\" in emb0 else emb0\n",
    "\n",
    "    # object (single)\n",
    "    if hasattr(resp, \"embedding\") and resp.embedding is not None:\n",
    "        return getattr(resp.embedding, \"values\", resp.embedding)\n",
    "\n",
    "    # object (batch)\n",
    "    if hasattr(resp, \"embeddings\") and resp.embeddings:\n",
    "        emb0 = resp.embeddings[0]\n",
    "        return getattr(emb0, \"values\", emb0)\n",
    "\n",
    "    # list variants\n",
    "    if isinstance(resp, list):\n",
    "        if not resp:\n",
    "            raise RuntimeError(\"Empty list embed response.\")\n",
    "        first = resp[0]\n",
    "        if isinstance(first, dict) and \"values\" in first:\n",
    "            return first[\"values\"]\n",
    "        if hasattr(first, \"values\"):\n",
    "            return first.values\n",
    "        if isinstance(first, list) and all(isinstance(x, (int, float)) for x in first):\n",
    "            return first\n",
    "        # flat list of floats already?\n",
    "        if all(isinstance(x, (int, float)) for x in resp):\n",
    "            return resp\n",
    "\n",
    "    raise RuntimeError(f\"Unexpected embed response type/shape: {type(resp)} -> {resp!r}\")\n",
    "\n",
    "\n",
    "def _to_1d(vec):\n",
    "    arr = np.array(vec, dtype=np.float32)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        return arr[0]  # pick first row if a batch sneaks in\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "\n",
    "def _embed_one(text: str, max_retries: int = 5, base_sleep: float = 5.0):\n",
    "    \"\"\"\n",
    "    Embed a single string using Gemini. Returns a 1-D np.ndarray.\n",
    "    Retries on 429/ResourceExhausted. Robust to SDK response variants.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=text)\n",
    "            emb = _extract_first_embedding(resp)\n",
    "            return _to_1d(emb)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"429\" in msg or \"ResourceExhausted\" in msg:\n",
    "                sleep_s = base_sleep * (attempt + 1)\n",
    "                print(f\"[embed] Quota hit (attempt {attempt+1}/{max_retries}). Sleeping {sleep_s}s...\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            print(f\"[embed] Unexpected response type: {type(resp) if 'resp' in locals() else None}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_adaptive_vs_standard_gemini' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    127\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# --- Run Gemini evaluation ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m evaluation_results = \u001b[43mevaluate_adaptive_vs_standard_gemini\u001b[49m(\n\u001b[32m    130\u001b[39m     pdf_paths=pdf_paths,\n\u001b[32m    131\u001b[39m     test_queries=test_queries,\n\u001b[32m    132\u001b[39m     reference_answers=reference_answers\n\u001b[32m    133\u001b[39m )\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# --- Print results nicely ---\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Gemini Evaluation Results ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluate_adaptive_vs_standard_gemini' is not defined"
     ]
    }
   ],
   "source": [
    "import time, json, hashlib, pathlib\n",
    "from collections import deque\n",
    "\n",
    "# --- Config knobs (tune to taste) ---\n",
    "MAX_GEN_PER_MIN = 12   # keep a safety margin under free-tier 15/min\n",
    "MAX_EMB_PER_MIN = 50   # under the 60/min embed guideline\n",
    "SLEEP_ON_SUCCESS = 0.5 # small gap between requests\n",
    "EMB_CACHE_DIR = pathlib.Path(\".cache/embeds\")\n",
    "GEN_CACHE_PATH = pathlib.Path(\".cache/generations.jsonl\")\n",
    "EMB_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GEN_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token-bucket-ish per-minute limiter using timestamps.\"\"\"\n",
    "    def __init__(self, per_minute: int):\n",
    "        self.per_minute = per_minute\n",
    "        self.events = deque()  # timestamps of last calls\n",
    "\n",
    "    def wait(self):\n",
    "        now = time.time()\n",
    "        # drop events older than 60s\n",
    "        while self.events and now - self.events[0] > 60:\n",
    "            self.events.popleft()\n",
    "        if len(self.events) >= self.per_minute:\n",
    "            sleep_s = 60 - (now - self.events[0]) + 0.05\n",
    "            time.sleep(max(sleep_s, 0.05))\n",
    "        # record this event\n",
    "        self.events.append(time.time())\n",
    "\n",
    "gen_limiter = RateLimiter(MAX_GEN_PER_MIN)\n",
    "emb_limiter = RateLimiter(MAX_EMB_PER_MIN)\n",
    "\n",
    "def _hash_key(*parts) -> str:\n",
    "    m = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        m.update(str(p).encode(\"utf-8\"))\n",
    "    return m.hexdigest()\n",
    "\n",
    "def _save_generation_cache(record: dict):\n",
    "    with GEN_CACHE_PATH.open(\"a\") as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _load_generation_cache():\n",
    "    cache = {}\n",
    "    if GEN_CACHE_PATH.exists():\n",
    "        with GEN_CACHE_PATH.open() as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    cache[rec[\"key\"]] = rec[\"text\"]\n",
    "                except:\n",
    "                    pass\n",
    "    return cache\n",
    "\n",
    "_GEN_CACHE = _load_generation_cache()\n",
    "\n",
    "def _gen_with_retry(prompt, temperature=0.0, max_retries=6, base_sleep=5):\n",
    "    \"\"\"Generation with rate-limit pacing, backoff, and disk cache.\"\"\"\n",
    "    key = _hash_key(\"gen\", prompt, temperature)\n",
    "    if key in _GEN_CACHE:\n",
    "        return _GEN_CACHE[key]\n",
    "\n",
    "    model = genai.GenerativeModel(GEN_MODEL)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            gen_limiter.wait()\n",
    "            resp = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "            )\n",
    "            text = (resp.text or \"\").strip()\n",
    "            _GEN_CACHE[key] = text\n",
    "            _save_generation_cache({\"key\": key, \"text\": text})\n",
    "            time.sleep(SLEEP_ON_SUCCESS)\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            # Parse suggested retry_delay if available\n",
    "            sleep_s = base_sleep * (attempt + 1)\n",
    "            try:\n",
    "                # The SDK often includes 'retry_delay { seconds: N }'\n",
    "                if \"retry_delay\" in msg:\n",
    "                    import re\n",
    "                    m = re.search(r\"retry_delay\\s*{\\s*seconds:\\s*(\\d+)\", msg)\n",
    "                    if m:\n",
    "                        sleep_s = max(sleep_s, int(m.group(1)) + 1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if \"ResourceExhausted\" in msg or \"429\" in msg:\n",
    "                # DAILY QUOTA?\n",
    "                if \"PerDay\" in msg or \"PerDayPerProjectPerModel\" in msg:\n",
    "                    raise RuntimeError(\n",
    "                        \"Daily generation quota exhausted. Try again after reset or upgrade.\"\n",
    "                    )\n",
    "                # Otherwise, wait and retry\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "def _embed_one_cached(text: str, max_retries=6, base_sleep=3):\n",
    "    \"\"\"Embedding with per-minute limiter, backoff, and disk cache.\"\"\"\n",
    "    key = _hash_key(\"emb\", text)\n",
    "    cache_path = EMB_CACHE_DIR / f\"{key}.npy\"\n",
    "    if cache_path.exists():\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            emb_limiter.wait()\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=text)\n",
    "            # robust normalization\n",
    "            emb = _extract_first_embedding(resp)  # <- use your robust parser\n",
    "            vec = _to_1d(emb)                     # <- ensure 1-D\n",
    "            np.save(cache_path, vec)\n",
    "            time.sleep(SLEEP_ON_SUCCESS)\n",
    "            return vec\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"ResourceExhausted\" in msg or \"429\" in msg:\n",
    "                # DAILY embed quota? (rare; usually per-minute)\n",
    "                if \"PerDay\" in msg:\n",
    "                    raise RuntimeError(\"Daily embedding quota exhausted.\")\n",
    "                time.sleep(base_sleep * (attempt + 1))\n",
    "                continue\n",
    "            raise\n",
    "# --- Run Gemini evaluation ---\n",
    "evaluation_results = evaluate_adaptive_vs_standard_gemini(\n",
    "    pdf_paths=pdf_paths,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# --- Print results nicely ---\n",
    "print(\"\\n=== Gemini Evaluation Results ===\")\n",
    "for i, res in enumerate(evaluation_results[\"results\"], start=1):\n",
    "    print(f\"\\n--- Query {i}: {res['query']} ---\")\n",
    "    print(f\"Query Type: {res['query_type']}\")\n",
    "    print(\"\\n[Standard Retrieval Response]:\")\n",
    "    print(res[\"standard_retrieval\"][\"response\"][:300], \"...\")\n",
    "    print(\"\\n[Adaptive Retrieval Response]:\")\n",
    "    print(res[\"adaptive_retrieval\"][\"response\"][:300], \"...\")\n",
    "    if \"reference_answer\" in res:\n",
    "        print(\"\\n[Reference Answer]:\")\n",
    "        print(res[\"reference_answer\"])\n",
    "\n",
    "if \"comparison\" in evaluation_results:\n",
    "    print(\"\\n=== Comparison Analysis ===\")\n",
    "    print(evaluation_results[\"comparison\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install google-generativeai python-dotenv pymupdf\n",
    "import os, time, json, hashlib, pathlib, re\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =========================\n",
    "# 1) Gemini setup\n",
    "# =========================\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set in your environment/.env\")\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "GEN_MODEL = \"gemini-2.0-flash\"\n",
    "EMBED_MODEL = \"models/embedding-001\"\n",
    "\n",
    "# =========================\n",
    "# 2) Rate limiting, backoff, and caches\n",
    "# =========================\n",
    "MAX_GEN_PER_MIN = 12    # below free-tier 15/min\n",
    "MAX_EMB_PER_MIN = 50    # below 60/min guideline\n",
    "SLEEP_ON_SUCCESS = 0.5  # small spacing between calls\n",
    "EMB_CACHE_DIR = pathlib.Path(\".cache/embeds\")\n",
    "GEN_CACHE_PATH = pathlib.Path(\".cache/generations.jsonl\")\n",
    "EMB_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GEN_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, per_minute: int):\n",
    "        self.per_minute = per_minute\n",
    "        self.events = deque()\n",
    "    def wait(self):\n",
    "        now = time.time()\n",
    "        while self.events and now - self.events[0] > 60:\n",
    "            self.events.popleft()\n",
    "        if len(self.events) >= self.per_minute:\n",
    "            sleep_s = 60 - (now - self.events[0]) + 0.05\n",
    "            time.sleep(max(sleep_s, 0.05))\n",
    "        self.events.append(time.time())\n",
    "\n",
    "gen_limiter = RateLimiter(MAX_GEN_PER_MIN)\n",
    "emb_limiter = RateLimiter(MAX_EMB_PER_MIN)\n",
    "\n",
    "def _hash_key(*parts) -> str:\n",
    "    m = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        m.update(str(p).encode(\"utf-8\"))\n",
    "    return m.hexdigest()\n",
    "\n",
    "def _save_generation_cache(record: dict):\n",
    "    with GEN_CACHE_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _load_generation_cache():\n",
    "    cache = {}\n",
    "    if GEN_CACHE_PATH.exists():\n",
    "        with GEN_CACHE_PATH.open(encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    cache[rec[\"key\"]] = rec[\"text\"]\n",
    "                except:\n",
    "                    pass\n",
    "    return cache\n",
    "\n",
    "_GEN_CACHE = _load_generation_cache()\n",
    "\n",
    "def _gen_with_retry(prompt, temperature=0.0, max_retries=6, base_sleep=5):\n",
    "    key = _hash_key(\"gen\", prompt, temperature)\n",
    "    if key in _GEN_CACHE:\n",
    "        return _GEN_CACHE[key]\n",
    "\n",
    "    model = genai.GenerativeModel(GEN_MODEL)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            gen_limiter.wait()\n",
    "            resp = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "            )\n",
    "            text = (resp.text or \"\").strip()\n",
    "            _GEN_CACHE[key] = text\n",
    "            _save_generation_cache({\"key\": key, \"text\": text})\n",
    "            time.sleep(SLEEP_ON_SUCCESS)\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            sleep_s = base_sleep * (attempt + 1)\n",
    "            # honor retry_delay if present\n",
    "            m = re.search(r\"retry_delay\\s*{\\s*seconds:\\s*(\\d+)\", msg)\n",
    "            if m:\n",
    "                sleep_s = max(sleep_s, int(m.group(1)) + 1)\n",
    "\n",
    "            if \"ResourceExhausted\" in msg or \"429\" in msg:\n",
    "                if \"PerDay\" in msg or \"PerDayPerProjectPerModel\" in msg:\n",
    "                    raise RuntimeError(\"Daily generation quota exhausted. Try after reset or upgrade.\")\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "def _extract_first_embedding(resp):\n",
    "    # dict single\n",
    "    if isinstance(resp, dict) and \"embedding\" in resp:\n",
    "        emb = resp[\"embedding\"]\n",
    "        return emb[\"values\"] if isinstance(emb, dict) and \"values\" in emb else emb\n",
    "    # dict batch\n",
    "    if isinstance(resp, dict) and \"embeddings\" in resp and resp[\"embeddings\"]:\n",
    "        emb0 = resp[\"embeddings\"][0]\n",
    "        return emb0[\"values\"] if isinstance(emb0, dict) and \"values\" in emb0 else emb0\n",
    "    # object single\n",
    "    if hasattr(resp, \"embedding\") and resp.embedding is not None:\n",
    "        return getattr(resp.embedding, \"values\", resp.embedding)\n",
    "    # object batch\n",
    "    if hasattr(resp, \"embeddings\") and resp.embeddings:\n",
    "        emb0 = resp.embeddings[0]\n",
    "        return getattr(emb0, \"values\", emb0)\n",
    "    # list variants\n",
    "    if isinstance(resp, list):\n",
    "        if not resp:\n",
    "            raise RuntimeError(\"Empty list embed response.\")\n",
    "        first = resp[0]\n",
    "        if isinstance(first, dict) and \"values\" in first:\n",
    "            return first[\"values\"]\n",
    "        if hasattr(first, \"values\"):\n",
    "            return first.values\n",
    "        if isinstance(first, list) and all(isinstance(x, (int, float)) for x in first):\n",
    "            return first\n",
    "        if all(isinstance(x, (int, float)) for x in resp):\n",
    "            return resp\n",
    "    raise RuntimeError(f\"Unexpected embed response type: {type(resp)} -> {resp!r}\")\n",
    "\n",
    "def _to_1d(vec):\n",
    "    arr = np.array(vec, dtype=np.float32)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        return arr[0]\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "def _embed_one_cached(text: str, max_retries=6, base_sleep=3):\n",
    "    key = _hash_key(\"emb\", text)\n",
    "    cache_path = EMB_CACHE_DIR / f\"{key}.npy\"\n",
    "    if cache_path.exists():\n",
    "        return np.load(cache_path)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            emb_limiter.wait()\n",
    "            resp = genai.embed_content(model=EMBED_MODEL, content=text)\n",
    "            emb = _extract_first_embedding(resp)\n",
    "            vec = _to_1d(emb)\n",
    "            np.save(cache_path, vec)\n",
    "            time.sleep(SLEEP_ON_SUCCESS)\n",
    "            return vec\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if \"ResourceExhausted\" in msg or \"429\" in msg:\n",
    "                if \"PerDay\" in msg:\n",
    "                    raise RuntimeError(\"Daily embedding quota exhausted.\")\n",
    "                time.sleep(base_sleep * (attempt + 1))\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# =========================\n",
    "# 3) PDF -> text -> chunks\n",
    "# =========================\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    out = []\n",
    "    for page in doc:\n",
    "        out.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks, start = [], 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        if end == len(text): break\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# =========================\n",
    "# 4) Tiny vector store (NumPy)\n",
    "# =========================\n",
    "class GeminiVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors, self.texts, self.metadata = [], [], []\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding, dtype=np.float32))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    def add_texts(self, texts, metadatas=None, sleep_between=0.25):\n",
    "        metadatas = metadatas or [{}]*len(texts)\n",
    "        for t, m in zip(texts, metadatas):\n",
    "            vec = _embed_one_cached(t)\n",
    "            self.add_item(t, vec, m)\n",
    "            if sleep_between:\n",
    "                time.sleep(sleep_between)\n",
    "    def similarity_search_by_text(self, query_text, k=4, filter_func=None):\n",
    "        q = _embed_one_cached(query_text)\n",
    "        qn = np.linalg.norm(q)\n",
    "        sims = []\n",
    "        for i, v in enumerate(self.vectors):\n",
    "            if filter_func and not filter_func(self.metadata[i]): \n",
    "                continue\n",
    "            vn = np.linalg.norm(v)\n",
    "            sim = 0.0 if qn == 0 or vn == 0 else float(np.dot(q, v) / (qn * vn))\n",
    "            sims.append((i, sim))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [{\"text\": self.texts[i], \"metadata\": self.metadata[i], \"similarity\": s}\n",
    "                for i, s in sims[:k]]\n",
    "\n",
    "# =========================\n",
    "# 5) Retrieval policies + generation\n",
    "# =========================\n",
    "def classify_query_gemini(query):\n",
    "    q = query.lower()\n",
    "    if any(x in q for x in [\"why\", \"how\", \"analyz\", \"compare\", \"tradeoff\"]): return \"Analytical\"\n",
    "    if any(x in q for x in [\"opinion\", \"should i\", \"what do you think\"]):   return \"Opinion\"\n",
    "    if any(x in q for x in [\"context\", \"background\", \"overview\", \"explain\"]): return \"Contextual\"\n",
    "    return \"Factual\"\n",
    "\n",
    "def adaptive_retrieval_gemini(query, vector_store, k=4):\n",
    "    qtype = classify_query_gemini(query)\n",
    "    kk = {\"Factual\": max(4, k), \"Contextual\": max(5, k), \"Analytical\": k, \"Opinion\": max(3, k)}.get(qtype, k)\n",
    "    return qtype, vector_store.similarity_search_by_text(query, k=kk)\n",
    "\n",
    "def generate_response_gemini(query, docs, query_type=\"General\"):\n",
    "    context = \"\\n\\n\".join([f\"[{i+1}] {d['text']}\" for i, d in enumerate(docs)])\n",
    "    prompt = f\"\"\"You are a helpful assistant answering with ONLY the provided context if possible.\n",
    "Query type: {query_type}\n",
    "User query: {query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Cite supporting chunk indices like [1], [2] in-line.\n",
    "- If the answer is not in context, say so briefly and provide best-effort guidance.\n",
    "- Be concise (120-180 words).\n",
    "\"\"\"\n",
    "    return _gen_with_retry(prompt, temperature=0.0)\n",
    "\n",
    "# =========================\n",
    "# 6) The missing function\n",
    "# =========================\n",
    "def evaluate_adaptive_vs_standard_gemini(pdf_paths, test_queries, reference_answers=None,\n",
    "                                         chunk_size=1200, overlap=250, k=4):\n",
    "    # Combine PDF text\n",
    "    all_texts = []\n",
    "    for p in pdf_paths:\n",
    "        try:\n",
    "            t = extract_text_from_pdf(p)\n",
    "            if t.strip():\n",
    "                all_texts.append(t)\n",
    "            else:\n",
    "                print(f\"Skipping empty/non-text PDF: {os.path.basename(p)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p}: {e}\")\n",
    "    combined = \"\\n\".join(all_texts)\n",
    "    if not combined.strip():\n",
    "        raise ValueError(\"No extractable text found in provided PDFs.\")\n",
    "\n",
    "    # Chunk + embed\n",
    "    chunks = chunk_text(combined, chunk_size=chunk_size, overlap=overlap)\n",
    "    vs = GeminiVectorStore()\n",
    "    vs.add_texts(chunks, sleep_between=0.25)  # throttle a bit\n",
    "\n",
    "    # Evaluate queries\n",
    "    results = []\n",
    "    for i, query in enumerate(test_queries):\n",
    "        # Standard\n",
    "        std_docs = vs.similarity_search_by_text(query, k=k)\n",
    "        std_resp = generate_response_gemini(query, std_docs, \"General\")\n",
    "\n",
    "        # Adaptive\n",
    "        qtype, adp_docs = adaptive_retrieval_gemini(query, vs, k=k)\n",
    "        adp_resp = generate_response_gemini(query, adp_docs, qtype)\n",
    "\n",
    "        entry = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": qtype,\n",
    "            \"standard_retrieval\": {\"documents\": std_docs, \"response\": std_resp},\n",
    "            \"adaptive_retrieval\": {\"documents\": adp_docs, \"response\": adp_resp},\n",
    "        }\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            entry[\"reference_answer\"] = reference_answers[i]\n",
    "        results.append(entry)\n",
    "\n",
    "    return {\"results\": results}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1 Build list of all PDF files in the folder\n",
    "pdf_folder = \"/Users/kekunkoya/Desktop/Revised RAG Project/PDFs\"\n",
    "pdf_paths = [\n",
    "    os.path.join(pdf_folder, fname)\n",
    "    for fname in os.listdir(pdf_folder)\n",
    "    if fname.lower().endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "test_queries = [\n",
    "    \"What should I do if I am unable to evacuate during an emergency?\"\n",
    "]\n",
    "reference_answers = [\n",
    "    \"Shelter in place in the safest room in your home, and inform your support network of your situation. Keep a radio or phone handy for updates from officials.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_adaptive_vs_standard_gemini(\n",
    "    pdf_paths=pdf_paths,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Gemini Evaluation Results ===\n",
      "\n",
      "--- Query 1: What should I do if I am unable to evacuate during an emergency? ---\n",
      "Query Type: Opinion\n",
      "\n",
      "[Standard Retrieval Response]:\n",
      "If you are unable to evacuate, you should plan to get inside, find a safe spot, and stay put until local officials say the threat has passed [1]. Identify a safe spot in your home to shelter in place with members of your household and your pets; the safest spot will depend on the type of disaster [1 ...\n",
      "\n",
      "[Adaptive Retrieval Response]:\n",
      "If you are unable to evacuate during an emergency, you should plan to get inside, find a safe spot, and stay put until local officials say the threat has passed [1]. Identify a safe spot in your home to shelter in place with members of your household and your pets; the safest spot will depend on the ...\n",
      "\n",
      "[Reference Answer]:\n",
      "Shelter in place in the safest room in your home, and inform your support network of your situation. Keep a radio or phone handy for updates from officials.\n"
     ]
    }
   ],
   "source": [
    "# Run Gemini evaluation\n",
    "evaluation_results = evaluate_adaptive_vs_standard_gemini(\n",
    "    pdf_paths=pdf_paths,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print results nicely\n",
    "print(\"\\n=== Gemini Evaluation Results ===\")\n",
    "for i, res in enumerate(evaluation_results[\"results\"], start=1):\n",
    "    print(f\"\\n--- Query {i}: {res['query']} ---\")\n",
    "    print(f\"Query Type: {res['query_type']}\")\n",
    "    \n",
    "    print(\"\\n[Standard Retrieval Response]:\")\n",
    "    print(res[\"standard_retrieval\"][\"response\"][:300], \"...\")\n",
    "    \n",
    "    print(\"\\n[Adaptive Retrieval Response]:\")\n",
    "    print(res[\"adaptive_retrieval\"][\"response\"][:300], \"...\")\n",
    "    \n",
    "    if \"reference_answer\" in res:\n",
    "        print(\"\\n[Reference Answer]:\")\n",
    "        print(res[\"reference_answer\"])\n",
    "\n",
    "if \"comparison\" in evaluation_results:\n",
    "    print(\"\\n=== Comparison Analysis ===\")\n",
    "    print(evaluation_results[\"comparison\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
