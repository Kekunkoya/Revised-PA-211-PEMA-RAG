{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems\n",
    "\n",
    "This notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information.\n",
    "\n",
    "## Key Transformation Techniques\n",
    "\n",
    "1. **Query Rewriting**: Makes queries more specific and detailed for better search precision.\n",
    "2. **Step-back Prompting**: Generates broader queries to retrieve useful contextual information.\n",
    "3. **Sub-query Decomposition**: Breaks complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Query Transformation Techniques\n",
    "### 1. Query Rewriting\n",
    "This technique makes queries more specific and detailed to improve precision in retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for query rewriting\n",
    "        \n",
    "    Returns:\n",
    "        str: The rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be rewritten\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the rewritten query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting\n",
    "This technique generates broader queries to retrieve contextual background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a more general 'step-back' query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for step-back query generation\n",
    "        \n",
    "    Returns:\n",
    "        str: The step-back query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be generalized\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the step-back query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the step-back query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition\n",
    "This technique breaks down complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original complex query\n",
    "        num_subqueries (int): Number of sub-queries to generate\n",
    "        model (str): The model to use for query decomposition\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be decomposed\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the sub-queries using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.4,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Process the response to extract sub-queries\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numbered queries using simple parsing\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # Remove the number and leading space\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Transformation Techniques\n",
    "Let's apply these techniques to an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Where can I find emergency food in ZIP code 17104?\n",
      "\n",
      "1. Rewritten Query:\n",
      "emergency food resources in 17104\n",
      "\n",
      "2. Step-back Query:\n",
      "What are the common methods for finding food assistance?\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. 1. What are local food banks in ZIP code 17104?\n",
      "   2. 2. Are there any food pantries in the Harrisburg, PA area?\n",
      "   3. 3. How can I apply for food assistance programs?\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Assume genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) has been run.\n",
    "# I'll define a helper function to make it easier.\n",
    "\n",
    "def call_gemini(prompt, model=\"gemini-pro\", temperature=0):\n",
    "    \"\"\"A helper function to make a call to the Gemini API.\"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "# --- Example query ---\n",
    "original_query = \"Where can I find emergency food in ZIP code 17104?\"\n",
    "\n",
    "# --- Function Definitions for Query Transformations ---\n",
    "\n",
    "def rewrite_query(query):\n",
    "    \"\"\"\n",
    "    Rewrites the user query to be more effective for keyword-based retrieval.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a query rewriting model. Your task is to rephrase the user's query\n",
    "    to be more concise and suitable for a search engine. Do not add or remove\n",
    "    any key information.\n",
    "\n",
    "    User Query: {query}\n",
    "\n",
    "    Rewritten Query:\n",
    "    \"\"\"\n",
    "    # This function would call your LLM\n",
    "    # return call_gemini(prompt)\n",
    "    \n",
    "    # Placeholder for demonstration\n",
    "    return \"emergency food resources in 17104\"\n",
    "\n",
    "def generate_step_back_query(query):\n",
    "    \"\"\"\n",
    "    Generates a step-back query to get more general context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. The user is asking a specific question.\n",
    "    What is the high-level, \"step-back\" question that would provide\n",
    "    the necessary context to answer the user's original question?\n",
    "\n",
    "    Original Query: {query}\n",
    "\n",
    "    Step-back Question:\n",
    "    \"\"\"\n",
    "    # This function would call your LLM\n",
    "    # return call_gemini(prompt)\n",
    "    \n",
    "    # Placeholder for demonstration\n",
    "    return \"What are the common methods for finding food assistance?\"\n",
    "\n",
    "def decompose_query(query, num_subqueries=3):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into a set of simpler sub-queries.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Decompose the following query into {num_subqueries} distinct and independent sub-queries.\n",
    "    List each sub-query on a new line, starting with a number followed by a period.\n",
    "\n",
    "    Original Query: {query}\n",
    "    \n",
    "    Decomposed Sub-queries:\n",
    "    \"\"\"\n",
    "    # This function would call your LLM\n",
    "    # raw_response = call_gemini(prompt)\n",
    "    # return raw_response.split('\\n')\n",
    "    \n",
    "    # Placeholder for demonstration\n",
    "    return [\n",
    "        \"1. What are local food banks in ZIP code 17104?\",\n",
    "        \"2. Are there any food pantries in the Harrisburg, PA area?\",\n",
    "        \"3. How can I apply for food assistance programs?\"\n",
    "    ]\n",
    "\n",
    "# --- Apply query transformations ---\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=3)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "To demonstrate how query transformations integrate with retrieval, let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized with 3 items.\n",
      "\n",
      "Searching for items similar to: 'find food assistance'\n",
      "\n",
      "Top 2 search results:\n",
      "  - Text: How can I find emergency food?\n",
      "    Similarity: 0.7839\n",
      "    Metadata: {'source': 'PA211_dataset'}\n",
      "  - Text: Where are the closest food banks in Harrisburg?\n",
      "    Similarity: 0.6575\n",
      "    Metadata: {'source': 'PA211_dataset'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Initialize Gemini client ---\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variable\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Configure the Gemini API client\n",
    "try:\n",
    "    genai.configure(api_key=api_key)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Gemini API configuration: {e}\")\n",
    "    exit()\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            # Handle the case where a vector might have a norm of 0 to prevent division by zero\n",
    "            norm_query = np.linalg.norm(query_vector)\n",
    "            norm_vector = np.linalg.norm(vector)\n",
    "            if norm_query == 0 or norm_vector == 0:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (norm_query * norm_vector)\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items\n",
    "\n",
    "# --- Example Usage with Gemini ---\n",
    "\n",
    "def create_gemini_embedding(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"Creates an embedding for a single text using Gemini.\"\"\"\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return response['embedding']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # 2. Add some items with their Gemini embeddings\n",
    "    text1 = \"How can I find emergency food?\"\n",
    "    emb1 = create_gemini_embedding(text1)\n",
    "    store.add_item(text1, emb1, metadata={\"source\": \"PA211_dataset\"})\n",
    "\n",
    "    text2 = \"Where are the closest food banks in Harrisburg?\"\n",
    "    emb2 = create_gemini_embedding(text2)\n",
    "    store.add_item(text2, emb2, metadata={\"source\": \"PA211_dataset\"})\n",
    "\n",
    "    text3 = \"What should I do during a flood?\"\n",
    "    emb3 = create_gemini_embedding(text3)\n",
    "    store.add_item(text3, emb3, metadata={\"source\": \"PEMA.pdf\"})\n",
    "\n",
    "    print(\"Vector store initialized with 3 items.\")\n",
    "\n",
    "    # 3. Perform a similarity search with a new query\n",
    "    query_text = \"find food assistance\"\n",
    "    query_embedding = create_gemini_embedding(query_text)\n",
    "\n",
    "    print(f\"\\nSearching for items similar to: '{query_text}'\")\n",
    "    search_results = store.similarity_search(query_embedding, k=2)\n",
    "\n",
    "    # 4. Print the search results\n",
    "    print(\"\\nTop 2 search results:\")\n",
    "    for result in search_results:\n",
    "        print(f\"  - Text: {result['text']}\")\n",
    "        print(f\"    Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"    Metadata: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Make sure to initialize your Gemini API key before calling this function\n",
    "# Example: genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def create_embeddings(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified Gemini model.\n",
    "\n",
    "    Args:\n",
    "    text (str or list[str]): The input text(s) for which embeddings are to be created.\n",
    "                                Can be a single string or a list of strings.\n",
    "    model (str): The model to be used for creating embeddings. Defaults to \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "    list[float] or list[list[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    # Gemini's embed_content can handle both a single string or a list of strings.\n",
    "    # The output format is a list of embeddings, even for a single input.\n",
    "    response = genai.embed_content(\n",
    "        model=model,\n",
    "        content=text\n",
    "    )\n",
    "\n",
    "    # If the original input was a string, return just the first embedding vector.\n",
    "    if isinstance(text, str):\n",
    "        return response['embedding']\n",
    "\n",
    "    # Otherwise, return all embedding vectors as a list of lists.\n",
    "    return response['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Includes a SimpleVectorStore class that uses NumPy for in-memory storage of text, embeddings, and metadata, enabling basic cosine similarity searches.\n",
    "It calculates cosine similarity between the query_vector and all stored vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# --- Initialize Gemini client ---\n",
    "# Ensure your API key is configured.\n",
    "# genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def extract_text_with_gemini(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using the Gemini model's document understanding capabilities.\n",
    "    This function sends the entire PDF content to the model with a prompt to extract all text.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(pdf_path):\n",
    "            return f\"Error: File not found at {pdf_path}\"\n",
    "\n",
    "        # Read the PDF file as bytes\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            pdf_data = pdf_file.read()\n",
    "\n",
    "        # Create a GenerativeModel instance\n",
    "        model = genai.GenerativeModel(\"gemini-pro-vision\") # Or another suitable multimodal model\n",
    "\n",
    "        # Create a prompt that instructs the model to extract all text\n",
    "        prompt = \"Extract all text from this document.\"\n",
    "\n",
    "        # Prepare the parts for the generate_content call\n",
    "        contents = [\n",
    "            prompt,\n",
    "            {\n",
    "                'mime_type': 'application/pdf',\n",
    "                'data': pdf_data\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Call the Gemini API to get the text\n",
    "        response = model.generate_content(contents)\n",
    "\n",
    "        # Return the extracted text\n",
    "        return response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example Usage:\n",
    "# pdf_file_path = \"your_document.pdf\"\n",
    "# extracted_text = extract_text_with_gemini(pdf_file_path)\n",
    "# print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency Name *\n",
      "Site Name *\n",
      "Service Name *\n",
      "Site Main Phone \n",
      "Number\n",
      "Service Eligibility\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Home Repair Program\n",
      "717-394-0793\n",
      "Based on annual gross income, \n",
      "according to family size; Available \n",
      "equity in home\n",
      "Housing and Repairs\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Public Infrastructure and \n",
      "Community Facilities Grant \n",
      "Administration\n",
      "717-394-0793\n",
      "Local municipalities outside the \n",
      "city of Lancaster\n",
      "\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Rental Housing Program\n",
      "717-394-0793\n",
      "Open to rental housing \n",
      "developers only; Properties must \n",
      "be located in Lancaster County, \n",
      "outside of Lancaster City\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Grant Administration\n",
      "717-394-0793\n",
      "Agencies that serve Lancaster \n",
      "county (non-city) residents\n",
      "\n",
      "Low-income families, elderly (62 \n",
      "years or older), and disabled \n",
      "persons who live or wish to live in \n",
      "Lancaster County\n",
      "Elizabethtown \n",
      "Community Housing \n",
      "and Outreach \n",
      "Services\n",
      "Community Place on \n",
      "Washington\n",
      "Enrichment Center\n",
      "717-361-0740\n",
      "Lancaster County residents\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Section 8 Housing Choice \n",
      "Vouchers\n",
      "717-394-0793\n",
      "\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Homebuyer Counseling\n",
      "Residents Borrowers that are \n",
      "considering a PHFA loan product \n",
      "and have a FICO credit score \n",
      "lower than 660 are required to \n",
      "complete a course prior to closing \n",
      "on their loan.\n",
      "Pennsylvania Utility Law \n",
      "Project (PULP) Hotline\n",
      "844-645-2500\n",
      "Serves individuals and families in \n",
      "Pennsylvania who are facing a \n",
      "utility shutoff or are already \n",
      "without service.\n",
      "Housing \n",
      "Discrimination \n",
      "Hotline\n",
      "Harrisburg\n",
      "Housing Discrimination \n",
      "Hotline\n",
      "No limitations or restrictions\n",
      "Regional Housing \n",
      "Legal Services\n",
      "Pennsylvania Utility Law \n",
      "Project Office, Harrisburg\n",
      "\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Homeowner's Emergency \n",
      "Mortgage Assistance \n",
      "Program (HEMAP)\n",
      "A) Residents of Pennsylvania who \n",
      "are homeowners with mortgage \n",
      "delinquencies caused by \n",
      "circumstances beyond their \n",
      "control B) Must not be in \n",
      "foreclosure C) Must have the \n",
      "ability to regain financial stability \n",
      "within 24 months.\n",
      "Manheim Central \n",
      "Food Pantry\n",
      "Manheim\n",
      "Food Pantry\n",
      "717-664-1097\n",
      "Based on family size\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Daytime Transition Center\n",
      "717-291-2261\n",
      "Homeless single indiviudals, can \n",
      "not accomodate families. \n",
      "Intended for those who are trying \n",
      "to make efforts to move to a more \n",
      "stabalized situation\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Clothing Bank\n",
      "717-291-2261\n",
      "Anyone in need; Individuals can \n",
      "obtain clothing every 60 days if \n",
      "needed\n",
      "Food\n",
      "\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Code Red: Extreme Heat \n",
      "Cooling Program\n",
      "717-291-2261\n",
      "Anyone in need\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Warming Center\n",
      "717-291-2261\n",
      "Anyone in need\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster City various \n",
      "locations\n",
      "Street Outreach\n",
      "Homeless individuals\n",
      "Midwest Food Bank \n",
      "of Pennsylvania\n",
      "Middletown\n",
      "Food Bank\n",
      "717-614-8095\n",
      "Must be a feeding program, such \n",
      "as a local 501(c)(3) food pantry or \n",
      "soup kitchen.\n",
      "Central Pennsylvania \n",
      "Food Bank\n",
      "Harrisburg\n",
      "SNAP Outreach Program\n",
      "717-564-1700\n",
      "Gross monthly income limits \n",
      "specific to a household size\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Food Pantry\n",
      "717-291-2261\n",
      "Anyone in need, must sign a self-\n",
      "declaration of need each year. \n",
      "Caseworkers picking up on behalf \n",
      "of a recipient must sign a Proxy \n",
      "form giving the caseworker/case \n",
      "manager permission to pick up for \n",
      "them\n",
      "\n",
      "Ferris Wheel Clothing \n",
      "Bank\n",
      "Lititz\n",
      "Clothing Bank\n",
      "717-799-1933\n",
      "Anyone in need\n",
      "Northern Lebanon \n",
      "Clothing Closet\n",
      "Northern Lebanon Clothing \n",
      "Closet\n",
      "Clothing bank\n",
      "717-454-3697\n",
      "Northern Lebanon School District \n",
      "residents who make 225% or less \n",
      "of the federal poverty level, or is in \n",
      "crisis\n",
      "Southern York County \n",
      "Clothing Bank\n",
      "Saint John the Baptist \n",
      "Catholic Church\n",
      "Clothing Bank\n",
      "717-235-2156\n",
      "Residents of York County\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Cambria County EMA\n",
      "Emergency Management, \n",
      "Cambria County\n",
      "814-472-2050\n",
      "Cambria County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Central Pennsylvania \n",
      "Food Bank\n",
      "Harrisburg\n",
      "Senior Monthly Food Boxes\n",
      "717-564-1700\n",
      "Must be age 60 years or older and \n",
      "be at or between 131% and 185% \n",
      "of Federal Poverty Income \n",
      "Guidelines\n",
      "Emergency\n",
      "Clothing\n",
      "\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Armstrong County EMA\n",
      "Emergency Management, \n",
      "Armstrong County\n",
      "724-548-3429\n",
      "Armstrong County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Westmoreland County EMA\n",
      "Emergency Management, \n",
      "Westmoreland County\n",
      "724-600-7300\n",
      "Westmoreland County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Indiana County EMA\n",
      "Emergency Management, \n",
      "Indiana County\n",
      "724-349-9300\n",
      "Indiana County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Somerset County EMA\n",
      "Emergency Management, \n",
      "Somerset County\n",
      "814-445-1515\n",
      "Somerset County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Fayette County EMA\n",
      "Emergency Management, \n",
      "Fayette County\n",
      "724-430-1277\n",
      "Fayette County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Washington County EMA\n",
      "Emergency Management, \n",
      "Washington County\n",
      "724-228-6911\n",
      "Washington County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "Butler County residents, business \n",
      "owners, stakeholders, and \n",
      "visitors affected by a disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Allegheny County EMA\n",
      "Emergency Management, \n",
      "Allegheny County\n",
      "412-473-2550\n",
      "Allegheny County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Greene County EMA\n",
      "Emergency Management, \n",
      "Greene County\n",
      "724-627-5387\n",
      "Greene County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Butler County EMA\n",
      "Emergency Management, \n",
      "Butler County\n",
      "724-284-5211\n",
      "\n",
      "Citizens' Ambulance \n",
      "Service\n",
      "Indiana\n",
      "Non-Emergency Medical \n",
      "Transportation\n",
      "724-349-5511\n",
      "Services available in Indiana \n",
      "County and portions of \n",
      "Armstrong, Clearfield, and \n",
      "Westmoreland counties. Call for \n",
      "additional information.\n",
      "No limitations or restrictions\n",
      "Agencies\n",
      "disaster.\n",
      "Citizens' Ambulance \n",
      "Service\n",
      "Indiana\n",
      "Paramedic/EMT and \n",
      "Ambulance Services\n",
      "724-349-5511\n",
      "Serves Indiana County, and \n",
      "portions of Armstrong, \n",
      "Westmoreland, and Clearfield \n",
      "counties.\n",
      "Coudersport \n",
      "Volunteer Ambulance \n",
      "Association\n",
      "Coudersport Volunteer \n",
      "Ambulance Association\n",
      "Coudersport Volunteer \n",
      "Ambulance Association\n",
      "814-274-7411\n",
      "Penn Township\n",
      "Citizens' Ambulance \n",
      "Service\n",
      "Indiana\n",
      "Child Car Seats and \n",
      "Inspections\n",
      "724-349-5511\n",
      "Serves Indiana County and \n",
      "portions of Armstrong, Clearfield, \n",
      "and Westmoreland counties. Call \n",
      "for additional details.\n",
      "Ambulance\n",
      "\n",
      "Noga Ambulance\n",
      "New Castle\n",
      "Non-Emergency Medical \n",
      "Transportation\n",
      "724-652-8300\n",
      "Residents of Ellwood City, New \n",
      "Castle, and New Wilmington\n",
      "Noga Ambulance\n",
      "New Castle\n",
      "Safety Programs and \n",
      "Trainings\n",
      "724-652-8300\n",
      "Varies by program; call for details.\n",
      "Penn Township \n",
      "Ambulance \n",
      "Association\n",
      "Irwin\n",
      "Child Passenger Safety Seats 724-744-4112\n",
      "Families with children age 10 and \n",
      "younger\n",
      "\n",
      "Taxonomy *\n",
      "Furnaces - BM-3000.0500-450.25\n",
      "Home Rehabilitation Loans - BH-\n",
      "3000.3550-360\n",
      "Home Maintenance and Minor \n",
      "Repair Grants/Loans - PH-\n",
      "3300.2740\n",
      "Weatherization Programs - BH-\n",
      "3000.1800-950\n",
      "Community Development Block \n",
      "Grant Agencies - TD-1100.1500\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "Redevelopment Programs - TB-\n",
      "7000\n",
      "\n",
      "Charities/Grantmaking \n",
      "Organizations - TD-1200\n",
      "Financial Management Support - \n",
      "TP-2100\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "Housing Development - TB-3000\n",
      "Redevelopment Programs - TB-\n",
      "7000\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "\n",
      "Section 8 Housing Choice \n",
      "Vouchers - BH-7000.4600-700\n",
      "Homeless Drop In Centers - BH-\n",
      "1800.3500\n",
      "Clothing Vouchers - BM-\n",
      "6500.1500-130\n",
      "Food Vouchers - BD-1800.2250\n",
      "Personal/Grooming Supplies - BM-\n",
      "6500.6500-650\n",
      "Consumer Complaints - DD-1500\n",
      "\n",
      "Housing Advocacy Groups - TD-\n",
      "1600.2800\n",
      "Utility Bill Payment Plan \n",
      "Negotiation Assistance - BV-\n",
      "8900.9120\n",
      "Utility Disconnection Protection - \n",
      "BV-8900.9220\n",
      "Homebuyer/Home Purchase \n",
      "Counseling - BH-3700.3000\n",
      "\n",
      "Foreclosure Prevention Loan \n",
      "Modification/Refinancing \n",
      "Programs - BH-3500.3400-300\n",
      "Food Pantries - BD-1800.2000\n",
      "Homeless Drop In Centers - BH-\n",
      "1800.3500\n",
      "General Clothing Donation \n",
      "Programs - TI-1800.1500-250\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "\n",
      "Food Donation Programs - TI-\n",
      "1800.2000\n",
      "Grocery Ordering/Delivery - BD-\n",
      "2400.2590\n",
      "Food Pantries - BD-1800.2000\n",
      "Extreme Heat Cooling Programs - \n",
      "TH-2600.1900\n",
      "Extreme Cold Warming Centers - \n",
      "TH-2600.1880\n",
      "Street Outreach Programs - PH-\n",
      "8000\n",
      "Food Banks/Food Distribution \n",
      "Warehouses - BD-1875.2000\n",
      "Benefits Screening - PH-0700\n",
      "\n",
      "Commodity Supplemental Food \n",
      "Program - BD-1800.1500\n",
      "Brown Bag Food Programs - BD-\n",
      "1800.1000\n",
      "Food Pantries - BD-1800.2000\n",
      "Food Lines - BD-1800.1900\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Paramedic/EMT Services - LD-\n",
      "6500\n",
      "Emergency Medical Transportation \n",
      "- LD-1500\n",
      "Emergency Medical Transportation \n",
      "- LD-1500\n",
      "Paramedic/EMT Services - LD-\n",
      "6500\n",
      "Non-Emergency Medical \n",
      "Transportation - BT-4500.6500-\n",
      "500\n",
      "Child Passenger Safety Seat \n",
      "Inspections - JR-8200.8500-160\n",
      "Child Passenger Safety Seats - JR-\n",
      "8400.1500\n",
      "Child Passenger Safety Education - \n",
      "JR-8200.8500-150\n",
      "\n",
      "Child Passenger Safety Seat \n",
      "Inspections - JR-8200.8500-160\n",
      "Child Passenger Safety Seats - JR-\n",
      "8400.1500\n",
      "Non-Emergency Medical \n",
      "Transportation - BT-4500.6500-\n",
      "500\n",
      "First Aid Instruction - LH-\n",
      "2700.2000\n",
      "Bicycle Safety Education - JR-\n",
      "8200.8500-100\n",
      "Child Passenger Safety Seat \n",
      "Inspections - JR-8200.8500-160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = \"//Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PA 211 Disaster Community Resources.pdf ---\n",
      "PA 211 Community Disaster and Human \n",
      "Services Resources in Pennsylvania \n",
      "Introduction \n",
      " \n",
      "Community Disaster and Human Services Resources in Pennsylvania \n",
      " \n",
      "Disasters, whether natural or man-made, have significant and far-reaching impacts on \n",
      "individuals, families, and communities. Pennsylvania, with its mix of urban, suburban, and \n",
      "rural regions, faces a diverse array of emergencies ranging from floods and severe storms to \n",
      "public health crises and housing instability. To ensure an effective res\n",
      "\n",
      "--- 211 RESPONDS TO URGENT NEEDS.pdf ---\n",
      "211 RESPONDS TO URGENT NEEDS \n",
      "FACT\n",
      "211 stood up a statewide text\n",
      "response to support employees\n",
      "impacted by the partial federal\n",
      "government shutdown who did\n",
      "not know when they would\n",
      "receive their next paycheck.\n",
      "211 assists in times of\n",
      "disaster and widespread\n",
      "need\n",
      "FACT\n",
      "FACT\n",
      "1\n",
      "PLEASE VOTE TO INCLUDE FUNDING FOR PENNSYLVANIA'S 211 SYSTEM IN THE STATE BUDGET TO\n",
      "SUPPORT 211'S CAPACITY TO HELP OUR COMMUNITIES IN TIMES OF DISASTER OR GREAT NEED.\n",
      "1 2-1-1 Data\n",
      "In January 2019, 187 individuals\n",
      "subscribed to\n",
      "\n",
      "--- PEMA.pdf ---\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESS\n",
      "GUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready.PA.gov \n",
      "readypa@pa.gov\n",
      "\n",
      "Emergency Preparedness Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page\n",
      "\n",
      "--- ready-gov_disaster-preparedness-guide-for-older-adults.pdf ---\n",
      "1\n",
      "TAKE  \n",
      "CONTROL IN\n",
      "1\n",
      "2\n",
      "3\n",
      "Disaster Preparedness Guide for Older Adults\n",
      "\n",
      " \n",
      " \n",
      "STEP 1 | ASSESS YOUR NEEDS\n",
      "First, know your risk. Then, understand your needs during emergencies. \n",
      "This section guides you through a self-assessment process to identify your \n",
      "specific needs so that you can create a personalized emergency plan.\n",
      "STEP 2 | MAKE A PLAN\n",
      "Develop a comprehensive emergency plan and emergency \n",
      "preparedness kit tailored to your unique needs. This section ensures \n",
      "you are well prepared to respond to\n",
      "\n",
      "--- Substantial Damages Toolkit.pdf ---\n",
      " \n",
      " \n",
      " \n",
      "Prepared for: \n",
      "Pennsylvania Emergency Management \n",
      "Agency \n",
      "Emergency Management, Mitigation, \n",
      "Insurance, and Resilient Communities \n",
      "(MIRC) Office \n",
      "1310 Elmerton Avenue  \n",
      "Harrisburg, Pennsylvania 17110 \n",
      "Prepared by: \n",
      "PG Environmental and ERG \n",
      "14555 Avion Parkway, Suite 125 \n",
      "Chantilly, VA 20151 \n",
      " \n",
      "Substantial Improvements /  \n",
      "Substantial Damages Toolkit \n",
      "APRIL 2023 \n",
      "\n",
      " \n",
      "ii \n",
      " \n",
      "Contents \n",
      "Introduction ................................................................................................\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_texts_from_folder(folder_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDF files in a folder (recursively).\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDFs.\n",
    "    Returns:\n",
    "        dict: {pdf_filename: extracted_text, ...}\n",
    "    \"\"\"\n",
    "    pdf_texts = {}\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    pdf_texts[pdf_path] = extract_text_from_pdf(pdf_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {pdf_path}: {e}\")\n",
    "    return pdf_texts\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"/Users/kekunkoya/Desktop/RAG Google 2/PDFs\"\n",
    "pdf_texts = extract_texts_from_folder(folder_path)\n",
    "\n",
    "for pdf, text in pdf_texts.items():\n",
    "    print(f\"\\n--- {os.path.basename(pdf)} ---\")\n",
    "    print(text[:500])  # Print the first 500 characters for preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "\n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG using Gemini-compatible helper functions.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    # This assumes a Gemini-compatible `extract_text_from_pdf` function\n",
    "    # is available, which might be a local library like PyMuPDF for efficiency.\n",
    "    # We will use the local PyPDF2-based function for this example.\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if not extracted_text:\n",
    "        print(\"Failed to extract text from the PDF. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Chunking text...\")\n",
    "    # The chunk_text function is generic and does not need modification.\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    # This assumes a Gemini-compatible `create_embeddings` function is available.\n",
    "    # It will use the Gemini `embed_content` API.\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "    # Create and populate the vector store.\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    if len(chunks) != len(chunk_embeddings):\n",
    "        print(\"Error: Mismatch between number of chunks and embeddings.\")\n",
    "        return None\n",
    "\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "\n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Standard Search ---\n",
      "Transformation type: none\n",
      "Original query: Where can I get food assistance in Harrisburg, PA?\n",
      "Text: How can I apply for SNAP benefits in Pennsylvania? (Similarity: 0.8337)\n",
      "Text: I need help with my utility bills in Harrisburg. (Similarity: 0.8133)\n",
      "\n",
      "--- Rewrite Search ---\n",
      "Transformation type: rewrite\n",
      "Original query: Where can I get food assistance in Harrisburg, PA?\n",
      "Rewritten query: Food assistance Harrisburg PA\n",
      "Text: I need help with my utility bills in Harrisburg. (Similarity: 0.7999)\n",
      "Text: How can I apply for SNAP benefits in Pennsylvania? (Similarity: 0.7880)\n",
      "\n",
      "--- Step-back Search ---\n",
      "Transformation type: step_back\n",
      "Original query: Where can I get food assistance in Harrisburg, PA?\n",
      "Step-back query: What are your specific needs and circumstances that make you ask about food assistance?\n",
      "Text: How can I apply for SNAP benefits in Pennsylvania? (Similarity: 0.7096)\n",
      "Text: What are the available food resources in zip code 17104? (Similarity: 0.7024)\n",
      "\n",
      "--- Decompose Search ---\n",
      "Transformation type: decompose\n",
      "Original query: Where can I get food assistance in Harrisburg, PA?\n",
      "Decomposed into sub-queries:\n",
      "1. 1.  What food assistance programs are available?\n",
      "2. 2.  What organizations provide food assistance in Harrisburg, PA?\n",
      "3. 3.  What are the addresses and contact information for food assistance providers?\n",
      "Text: What are the available food resources in zip code 17104? (Similarity: 0.7961)\n",
      "Text: I need help with my utility bills in Harrisburg. (Similarity: 0.7955)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Helper functions and classes from previous conversions ---\n",
    "\n",
    "# A simple vector store class (Gemini-agnostic)\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            norm_query = np.linalg.norm(query_vector)\n",
    "            norm_vector = np.linalg.norm(vector)\n",
    "            if norm_query == 0 or norm_vector == 0:\n",
    "                similarity = 0.0\n",
    "            else:\n",
    "                similarity = np.dot(query_vector, vector) / (norm_query * norm_vector)\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Gemini API helper function to create embeddings\n",
    "def create_embeddings(text, model=\"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified Gemini model.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        response = genai.embed_content(model=model, content=text)\n",
    "        return response['embedding']\n",
    "    \n",
    "    # If text is a list, create embeddings for all of them\n",
    "    response = genai.embed_content(model=model, content=text)\n",
    "    return response['embedding']\n",
    "\n",
    "# A helper function to call the Gemini API\n",
    "def call_gemini(prompt, model=\"gemini-2.0-flash\", temperature=0):\n",
    "    \"\"\"A helper function to make a call to the Gemini API.\"\"\"\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(temperature=temperature)\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "# Query Rewriting with Gemini\n",
    "def rewrite_query(query):\n",
    "    \"\"\"\n",
    "    Rewrites the user query to be more effective for keyword-based retrieval.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a query rewriting model. Your task is to rephrase the user's query\n",
    "    to be more concise and suitable for a search engine. Do not add or remove\n",
    "    any key information.\n",
    "\n",
    "    User Query: {query}\n",
    "\n",
    "    Rewritten Query:\n",
    "    \"\"\"\n",
    "    return call_gemini(prompt)\n",
    "\n",
    "# Step-back Prompting with Gemini\n",
    "def generate_step_back_query(query):\n",
    "    \"\"\"\n",
    "    Generates a step-back query to get more general context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. The user is asking a specific question.\n",
    "    What is the high-level, \"step-back\" question that would provide\n",
    "    the necessary context to answer the user's original question?\n",
    "\n",
    "    Original Query: {query}\n",
    "\n",
    "    Step-back Question:\n",
    "    \"\"\"\n",
    "    return call_gemini(prompt)\n",
    "\n",
    "# Sub-query Decomposition with Gemini\n",
    "def decompose_query(query, num_subqueries=3):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into a set of simpler sub-queries.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Decompose the following query into {num_subqueries} distinct and independent sub-queries.\n",
    "    List each sub-query on a new line, starting with a number followed by a period.\n",
    "\n",
    "    Original Query: {query}\n",
    "    \n",
    "    Decomposed Sub-queries:\n",
    "    \"\"\"\n",
    "    raw_response = call_gemini(prompt)\n",
    "    # The response is a string, so we split it by lines\n",
    "    sub_queries = [line.strip() for line in raw_response.split('\\n') if line.strip()]\n",
    "    return sub_queries\n",
    "\n",
    "# --- The core function, unchanged in its logic ---\n",
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    Search using a transformed query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with rewritten query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # Step-back prompting\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with step-back query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # Sub-query decomposition\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # Create embeddings for all sub-queries\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # Search with each sub-query and combine results\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # Remove duplicates (keep highest similarity score)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # Sort by similarity and take top_k\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # Regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # You would need to load your data and create a vector store first.\n",
    "    # For this example, we'll create a simple dummy store.\n",
    "    \n",
    "    load_dotenv()\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    dummy_chunks = [\n",
    "        \"What are the available food resources in zip code 17104?\",\n",
    "        \"How can I apply for SNAP benefits in Pennsylvania?\",\n",
    "        \"Where can I find a homeless shelter?\",\n",
    "        \"What are the procedures for a flash flood warning?\",\n",
    "        \"I need help with my utility bills in Harrisburg.\",\n",
    "    ]\n",
    "    \n",
    "    for chunk in dummy_chunks:\n",
    "        emb = create_embeddings(chunk)\n",
    "        vector_store.add_item(chunk, emb)\n",
    "\n",
    "    # --- Run an example search ---\n",
    "    example_query = \"Where can I get food assistance in Harrisburg, PA?\"\n",
    "    \n",
    "    print(\"--- Standard Search ---\")\n",
    "    standard_results = transformed_search(example_query, vector_store, \"none\", top_k=2)\n",
    "    for res in standard_results:\n",
    "        print(f\"Text: {res['text']} (Similarity: {res['similarity']:.4f})\")\n",
    "\n",
    "    print(\"\\n--- Rewrite Search ---\")\n",
    "    rewrite_results = transformed_search(example_query, vector_store, \"rewrite\", top_k=2)\n",
    "    for res in rewrite_results:\n",
    "        print(f\"Text: {res['text']} (Similarity: {res['similarity']:.4f})\")\n",
    "\n",
    "    print(\"\\n--- Step-back Search ---\")\n",
    "    step_back_results = transformed_search(example_query, vector_store, \"step_back\", top_k=2)\n",
    "    for res in step_back_results:\n",
    "        print(f\"Text: {res['text']} (Similarity: {res['similarity']:.4f})\")\n",
    "\n",
    "    print(\"\\n--- Decompose Search ---\")\n",
    "    decompose_results = transformed_search(example_query, vector_store, \"decompose\", top_k=2)\n",
    "    for res in decompose_results:\n",
    "        print(f\"Text: {res['text']} (Similarity: {res['similarity']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response with Transformed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def generate_response(query, context, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and retrieved context using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): The model to use for response generation. Defaults to \"gemini-pro\".\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # For Gemini, it's often best to combine the system prompt with the user's prompt\n",
    "    # to guide the model's behavior, as a dedicated 'system' role isn't universally\n",
    "    # supported in the same way as in the OpenAI Chat Completions API.\n",
    "\n",
    "    # Combine the system prompt with the user's query and context.\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Gemini GenerativeModel\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate the response using the specified model.\n",
    "    # We pass the combined prompt and set the generation temperature.\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.1 # A low temperature for deterministic output\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Access the generated text from the response object\n",
    "        return response.text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle potential errors, such as a model not being able to generate a response\n",
    "        return f\"An error occurred while generating the response: {e}\"\n",
    "\n",
    "# Example Usage (assuming you have context and query strings):\n",
    "# context_text = \"This is a document about the company's new policy on remote work. Remote work is allowed for all employees, provided they have a stable internet connection and get approval from their manager.\"\n",
    "# user_query = \"What is the policy on remote work?\"\n",
    "#\n",
    "# response = generate_response(user_query, context_text)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rag_with_query_transformation_for_folder(folder_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation on all PDFs in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing PDF documents\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Results per PDF, each including filename, query, transformation, context, and response\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing: {pdf_path}\")\n",
    "\n",
    "                # Process the document to create a vector store\n",
    "                vector_store = process_document(pdf_path)\n",
    "\n",
    "                # Apply query transformation and search\n",
    "                if transformation_type:\n",
    "                    search_results = transformed_search(query, vector_store, transformation_type)\n",
    "                else:\n",
    "                    query_embedding = create_embeddings(query)\n",
    "                    search_results = vector_store.similarity_search(query_embedding, k=3)\n",
    "\n",
    "                # Combine context from search results\n",
    "                context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(search_results)])\n",
    "\n",
    "                # Generate response based on the query and combined context\n",
    "                response = generate_response(query, context)\n",
    "\n",
    "                # Store results for this PDF\n",
    "                results.append({\n",
    "                    \"pdf_file\": file,\n",
    "                    \"original_query\": query,\n",
    "                    \"transformation_type\": transformation_type,\n",
    "                    \"context\": context,\n",
    "                    \"response\": response\n",
    "                })\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rag_with_query_transformation_folder(folder_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation using Gemini-compatible functions on all PDFs in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing PDF documents\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: Results per PDF, each including file, query, context, and response (or error)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"\\n=== Processing: {pdf_path} ===\")\n",
    "                try:\n",
    "                    # Process the document to create a vector store\n",
    "                    print(\"Starting RAG pipeline...\")\n",
    "                    vector_store = process_document(pdf_path)\n",
    "                except Exception as e:\n",
    "                    results.append({\"pdf_file\": file, \"error\": f\"Failed to process document: {e}\"})\n",
    "                    continue\n",
    "\n",
    "                if vector_store is None:\n",
    "                    results.append({\"pdf_file\": file, \"error\": \"Vector store could not be created.\"})\n",
    "                    continue\n",
    "\n",
    "                # Apply query transformation and search\n",
    "                if transformation_type and transformation_type in ['rewrite', 'step_back', 'decompose']:\n",
    "                    print(f\"Applying '{transformation_type}' transformation to the query.\")\n",
    "                    try:\n",
    "                        search_results = transformed_search(query, vector_store, transformation_type)\n",
    "                    except Exception as e:\n",
    "                        results.append({\"pdf_file\": file, \"error\": f\"Transformed search failed: {e}\"})\n",
    "                        continue\n",
    "                else:\n",
    "                    print(\"Performing a standard search without query transformation.\")\n",
    "                    try:\n",
    "                        query_embedding = create_embeddings(query)\n",
    "                        search_results = vector_store.similarity_search(query_embedding, k=3)\n",
    "                    except Exception as e:\n",
    "                        results.append({\"pdf_file\": file, \"error\": f\"Standard search failed: {e}\"})\n",
    "                        continue\n",
    "\n",
    "                # Combine context from search results\n",
    "                context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(search_results)])\n",
    "                \n",
    "                # Generate response based on the query and combined context\n",
    "                try:\n",
    "                    print(\"Generating final response...\")\n",
    "                    response = generate_response(query, context)\n",
    "                except Exception as e:\n",
    "                    results.append({\"pdf_file\": file, \"error\": f\"Response generation failed: {e}\"})\n",
    "                    continue\n",
    "\n",
    "                print(\"Pipeline complete.\")\n",
    "                results.append({\n",
    "                    \"pdf_file\": file,\n",
    "                    \"original_query\": query,\n",
    "                    \"transformation_type\": transformation_type,\n",
    "                    \"context\": context,\n",
    "                    \"response\": response\n",
    "                })\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def compare_responses(results, reference_answer, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Compare responses from different query transformation techniques using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        results (Dict): Results from different transformation techniques\n",
    "        reference_answer (str): Reference answer for comparison\n",
    "        model (str): The model to use for evaluation. Defaults to \"gemini-2.0-flash\".\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "\n",
    "    # Prepare the comparison text with the reference answer and responses from each technique\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        # Ensure the 'response' key exists and is a string\n",
    "        response_content = result.get('response', 'No response found.')\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{response_content}\\n\\n\"\n",
    "    \n",
    "    # Define the user prompt with the comparison text\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (e.g., original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine the system prompt and user prompt for the Gemini API call\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    # Generate the evaluation response using the specified model\n",
    "    try:\n",
    "        model_instance = genai.GenerativeModel(model)\n",
    "        response = model_instance.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.1 # Low temperature for deterministic output\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Print the evaluation results\n",
    "        print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "        print(response.text.strip())\n",
    "        print(\"=============================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during evaluation: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def evaluate_transformations_folder(folder_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate different transformation techniques for the same query on all PDFs in a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing PDF documents\n",
    "        query (str): Query to evaluate\n",
    "        reference_answer (str): Optional reference answer for comparison\n",
    "\n",
    "    Returns:\n",
    "        Dict: Results per PDF, each with results for each transformation type\n",
    "    \"\"\"\n",
    "    # Transformation types to evaluate\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    all_results = {}\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"\\n\\n===== Evaluating PDF: {pdf_path} =====\")\n",
    "                results = {}\n",
    "\n",
    "                for transformation_type in transformation_types:\n",
    "                    type_name = transformation_type if transformation_type else \"original\"\n",
    "                    print(f\"\\n--- Running RAG with {type_name} query ---\")\n",
    "\n",
    "                    # Run RAG for the transformation type\n",
    "                    result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "                    results[type_name] = result\n",
    "\n",
    "                    # Print response or error\n",
    "                    if \"response\" in result:\n",
    "                        print(f\"Response with {type_name} query:\\n{result['response'][:400]}\")  # Preview\n",
    "                    else:\n",
    "                        print(f\"Error with {type_name} query: {result.get('error')}\")\n",
    "                    print(\"-\" * 50)\n",
    "                \n",
    "                # Optionally compare responses if a reference is given\n",
    "                if reference_answer:\n",
    "                    compare_responses(results, reference_answer)\n",
    "                \n",
    "                # Save all results for this PDF\n",
    "                all_results[file] = results\n",
    "\n",
    "    return all_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_paths, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate different transformation techniques for the same query using Gemini-compatible functions.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): Query to evaluate\n",
    "        reference_answer (str): Optional reference answer for comparison\n",
    "\n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Define the transformation techniques to evaluate\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # Run RAG with each transformation technique\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "        # Get the result for the current transformation type\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # --- CORRECTED CODE STARTS HERE ---\n",
    "        # Check for an error key in the result before trying to access 'response'\n",
    "        if \"error\" in result:\n",
    "            print(f\"Error during RAG with {type_name} query: {result['error']}\")\n",
    "        else:\n",
    "            # Print the response for the current transformation type\n",
    "            print(f\"Response with {type_name} query:\")\n",
    "            print(result[\"response\"])\n",
    "        # --- CORRECTED CODE ENDS HERE ---\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Compare results if a reference answer is provided\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== FINAL EVALUATION REPORT =====\n",
      "\n",
      "--- ORIGINAL QUERY RESULTS ---\n",
      "  Original Query: What is in the emergency kit?\n",
      "  Transformation Type: None\n",
      "\n",
      "  Generated Response:\n",
      "  The emergency kit contains food, water, and first-aid supplies.\n",
      "\n",
      "  Retrieved Context:\n",
      "  This is a test context about the emergency kit.\n",
      "\n",
      "--- REWRITE QUERY RESULTS ---\n",
      "  Original Query: What is in the emergency kit?\n",
      "  Transformation Type: rewrite\n",
      "\n",
      "  Generated Response:\n",
      "  The emergency kit contains food, water, and first-aid supplies.\n",
      "\n",
      "  Retrieved Context:\n",
      "  This is a test context about the emergency kit.\n",
      "\n",
      "\n",
      "===== END OF REPORT =====\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    \"\"\"\n",
    "    Prints the evaluation results from different RAG query transformation techniques.\n",
    "\n",
    "    Args:\n",
    "        results (Dict): The dictionary containing evaluation results for each technique.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n===== FINAL EVALUATION REPORT =====\")\n",
    "    \n",
    "    # Check if any results were produced\n",
    "    if not results:\n",
    "        print(\"No evaluation results to display.\")\n",
    "        return\n",
    "\n",
    "    # Print a summary for each transformation type\n",
    "    for technique, result in results.items():\n",
    "        print(f\"\\n--- {technique.upper()} QUERY RESULTS ---\")\n",
    "        \n",
    "        # Check for and handle errors\n",
    "        if \"error\" in result:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Original Query: {result.get('original_query', 'N/A')}\")\n",
    "        print(f\"  Transformation Type: {result.get('transformation_type', 'N/A')}\")\n",
    "        print(\"\\n  Generated Response:\")\n",
    "        print(\"  \" + result.get('response', 'No response generated.').replace('\\n', '\\n  '))\n",
    "        \n",
    "        # You may also want to print the context to understand why the response was generated\n",
    "        print(\"\\n  Retrieved Context:\")\n",
    "        print(\"  \" + result.get('context', 'No context retrieved.').replace('\\n', '\\n  '))\n",
    "\n",
    "    print(\"\\n\\n===== END OF REPORT =====\")\n",
    "\n",
    "# --- Example Usage (assuming 'evaluation_results' is populated) ---\n",
    "# Example dictionary structure (replace with your actual results)\n",
    "evaluation_results = {\n",
    "    'original': {\n",
    "        'original_query': 'What is in the emergency kit?',\n",
    "        'transformation_type': None,\n",
    "        'context': 'This is a test context about the emergency kit.',\n",
    "        'response': 'The emergency kit contains food, water, and first-aid supplies.'\n",
    "    },\n",
    "    'rewrite': {\n",
    "        'original_query': 'What is in the emergency kit?',\n",
    "        'transformation_type': 'rewrite',\n",
    "        'context': 'This is a test context about the emergency kit.',\n",
    "        'response': 'The emergency kit contains food, water, and first-aid supplies.'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Call the print function\n",
    "print_evaluation_results(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
